\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}
\usepackage{xcolor}         % colors


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2020} with \usepackage[nohyperref]{icml2020} above.
\usepackage{hyperref}
\usepackage{times}		
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} 
\DeclareUnicodeCharacter{00A0}{~}
\usepackage{amssymb,amsmath,amscd,amsfonts,amsthm,bbm,mathrsfs,yhmath}

\usepackage[shortlabels]{enumitem}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{graphics}
\usepackage{mathtools}
\usepackage{cleveref}
\usepackage{comment}

\usepackage{paralist}                                       
%\usepackage{tabto}
\usepackage{pdfpages}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{condition}{Condition}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}

\newcommand{\subscript}[2]{$#1 _ #2$}
\newlist{assumplist}{enumerate}{1}
\setlist[assumplist]{label=(\subscript{\textbf{A}}{{\arabic*}})}
\Crefname{assumplisti}{Assumption}{Assumptions}


\newlist{assumplist2}{enumerate}{1}
\setlist[assumplist2]{label=(\subscript{\textbf{B}}{{\arabic*}})}
\setcounter{assumption}{3}
\Crefname{assumplist2i}{Assumption}{Assumptions}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}



\usepackage[textwidth=2cm, textsize=footnotesize]{todonotes}  
%\setlength{\marginparwidth}{1.5cm}               %  this goes with todonoteshttps://fr.overleaf.com/project/5cc847f1dc86b619ccf2295b
\newcommand{\prnote}[1]{\todo[color=cyan!20]{#1}}
\newcommand{\asnote}[1]{\todo[color=green!]{#1}}
\newcommand{\lsnote}[1]{\todo[color=magenta]{#1}}
\newcommand{\adil}[1]{{\color{green!} #1}}
\newcommand{\cF}{{\mathcal F}}
\newcommand{\cB}{{\mathcal B}}
\newcommand{\cO}{{\mathcal O}}
\newcommand{\cG}{{\mathcal G}}
\newcommand{\cP}{{\mathcal P}}
\newcommand{\X}{{\mathcal X}} 
\newcommand{\Y}{{\mathcal Y}} 
\newcommand{\F}{{\mathcal F}}
\newcommand{\cH}{{\mathcal H}}
\newcommand{\J}{{\mathcal J}}
\newcommand{\G}{{\mathcal G}}
\newcommand{\C}{{\mathcal C}} 
\newcommand{\cS}{{\mathcal S}} 
\newcommand{\M}{{\mathcal M}} 
\newcommand{\E}{{{\mathbb E}}}
\newcommand{\kH}{{{\mathcal H}}} 
\newcommand{\R}{{\mathbb R}} 
\newcommand{\bP}{{\mathbb P}} 
\newcommand{\bE}{{\mathbb E}} 
\newcommand{\sX}{{\mathsf X}} 
\newcommand{\esp}{{\epsilon}} 
\newcommand{\Norm}[1]{\left\|#1\right\|_{H}}
\newcommand{\sqN}[1]{\Norm{#1}^2}

\newcommand{\dom}{{dom}} 
\newcommand{\KL}{\mathop{\mathrm{KL}}\nolimits}
\newcommand{\HS}{\mathop{\mathrm{HS}}\nolimits}
\newcommand{\op}{\mathop{\mathrm{op}}\nolimits}
\newcommand{\tr}{\mathop{\mathrm{tr}}\nolimits}
\newcommand{\st}{\mathop{\mathrm{Stein}}\nolimits}
\newcommand{\ps}[1]{\langle #1 \rangle}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}



\title{Formatting Instructions For NeurIPS 2021}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
  
\end{abstract}

\section{Copy-Paste from Adil's paper}
In this section, we analyze SVGD in discrete time, in the infinite number of particles regime~\eqref{eq:svgd_update}. We propose to study the dissipation of the KL along the SVGD algorithm. The Stein Fisher Information once again  quantifies this dissipation, as in the continuous time case. Before going further, note that discrete time analyses often require more assumptions that continuous time analyses. In optimization, these assumptions typically require some smoothness of the objective function. Here, we assume the following. 
%In this section we study the behavior of the Kullback-Leibler divergence along the Stein Variational Gradient Descent algorithm in discrete time.
\newcounter{contlist}
\begin{assumplist}
	\setlength\itemsep{0.2em}
		\item \label{ass:k_bounded}
	Assume that $\exists B>0$ s.t. for all $x \in \X$,\\
	$\|k(x,.)\|_{\cH_0}\le B$ and $\|\nabla _xk(x,.)\|_{\cH}=(\sum_{i=1}^d \|\partial_{x_i}k(x_i,.)\|^2_{\cH_0})^{\frac{1}{2}}\le B$.
	\item 	\label{ass:V_Lipschitz} The Hessian  $H_V$ of $V=-\log\pi$ is well-defined and $\exists M>0$ s.t. $\|H_V\|_{op}\le M$.
	%Assume  that $\nabla \log \pi$ is $M$-Lipschitz : \\$\|\nabla \log \pi (x) - \nabla \log \pi(y)\|\le M \|x-y\|$ for any $x,y\in \X$.
	\item \label{ass:bounded_I_Stein}	Assume  that $\exists$ is $C>0$ s.t.  $I_{Stein}(\mu_n|\pi)<C$ for all $n$.

\end{assumplist}
\setcounter{contlist}{\value{enumi}}



%In this section we assume \Cref{ass:k_bounded} holds, i.e. the norms of the kernel $k(x,x)$ and $  \nabla_1.\nabla_2 k(x,x)$ are bounded by some positive constant $B^2$.  We will also rely on \Cref{ass:bounded_I_Stein}, that states that the Stein Fisher information remains bounded at all iterations by some $C>0$.

%\subsection{A descent lemma}

%The following lemma states that the boundedness of the kernel, its gradient and the Hessian of $\pi$, as well as a the moments along the trajectory, are sufficient to satisfy the boundedness of the Stein Fisher information for all $n\ge0$. 

 Under \Cref{ass:V_Lipschitz,ass:k_bounded}, a sufficient condition for~\Cref{ass:bounded_I_Stein} is $\sup_n \int \Vert x \Vert \mu_n(x)dx < \infty$. Bounded moment assumptions such as these are commonly used in stochastic optimization, for instance in some analysis of the stochastic gradient descent~\citep{moulines2011non}. Given our assumptions, we quantify the decreasing of the KL along the SVGD algorithm, also called a descent lemma in optimization.
 


\begin{proposition}\label{prop:descent}
	%Define $\mu_{n+1} =   (I-\gamma K \nabla f_{\pi,\mu_n}   )_{\#}\mu_n $ and 
 Assume that \Cref{ass:V_Lipschitz,ass:k_bounded,ass:bounded_I_Stein} hold. % and that $\int \Vert x \Vert \mu_n(x)dx $ remains bounded for all $n$.
		  Let $\alpha>1$ and choose $ \gamma \leq \frac{\alpha-1}{\alpha BC^{\frac{1}{2}} } $. Then:
	\begin{align}\label{eq:descent}
	\KL(\mu_{n+1}|\pi)-\KL(\mu_{n}|\pi)\leq -\gamma \left(1- \gamma \frac{(\alpha^2 + M)B^2}{2}\right)I_{stein}(\mu_n|\pi).
	\end{align}
\end{proposition}
%  We provide here a sketch of the proof and the main ideas, the reader may refer to the \Cref{sec:proof_descent}  %\Cref{sec:proof_descent} 
%  for the complete proof.
\begin{proof}
    %	Let $\cF:\cP_2(\X)\to \R$ defined by $\cF(\mu)=KL(\mu|\pi)$.
    Our goal is to prove a discrete dissipation of the form $(\KL(\mu_{n+1}|\pi)-\KL(\mu_{n}|\pi))/\gamma \leq -I_{stein}(\mu_n|\pi) + \text{error term}$. 
    %\begin{equation}
        %$\frac{KL(\mu_{n+1}|\pi)-KL(\mu_{n}|\pi)}{\gamma} \leq -I_{stein}(\mu_n|\pi) + \text{error term}$. 
   % \end{equation}    
    Our assumptions will control the error term. 
    Fix $n \ge 0$ and denote $g = P_{\mu_n}\nabla \log(\frac{\mu_n}{\pi})$, $\phi_t = I - t g$ for $t \in [0,\gamma]$ and $\rho_t = (\phi_t)_{\#}\mu_n$. Note that $\rho_0 = \mu_n$ and $\rho_{\gamma} = \mu_{n+1}$. 
    
    Under our assumptions, one can show that for any $x \in \X$, $\|g(x)\|^2\le B^2 I_{Stein}(\mu_n|\pi)$ and $\|Jg(x)\|_{HS}^2 \le B^2 I_{Stein}(\mu_n|\pi)$, using the reproducing property and Cauchy-Schwartz in $\cH$. Hence, $\|t Jg(x)\|_{op} < 1$ and $\phi_t$ is a diffeomorphism for every $t \in [0,\gamma]$. Moreover, $\|(J \phi_t)^{-1}(x)\|_{op} \leq \alpha$. Using~\cite[Theorem 5.34]{villani2003topics}, the velocity field ruling the time evolution of $\rho_t$ is $w_t \in L^2(\rho_t)$ defined by $w_t(x) = -g(\phi_t^{-1}(x))$. %Note that $w_0 = -g\in \cH$.    
    
    Denote $\varphi(t) = \KL(\rho_t|\pi)$. Using a Taylor expansion,
        $\varphi(\gamma) = \varphi(0) + \gamma \varphi'(0) + \int_{0}^{\gamma} (\gamma - t)\varphi''(t)dt$.
        We now identify each term. First, 
        \begin{equation*}
    \varphi(0) = \KL(\mu_n|\pi)\; \text{ and } \;\varphi(\gamma) = \KL(\mu_{n+1}|\pi).
        \end{equation*} 
        Then, using the chain rule~\cite[Section 8.2]{villani2003topics}, 
        \begin{equation*}
            \varphi'(t)=\ps{\nabla_{W_2} \KL(\rho_t|\pi),w_t}_{L^2(\rho_t)}\; \text{ and } \;\varphi''(t) = \ps{w_t,Hess_{\KL(.|\pi)}(\rho_t)w_t}_{L^2(\rho_t)}.
        \end{equation*}
        Therefore, $\varphi'(0) = -\ps{\nabla \log\left(\frac{\mu_n}{\pi}\right),g}_{L^2(\mu_n)}%=-\ps{g,g}_{\cH}
        =-I_{Stein}(\mu_n|\pi)$. %, using $g \in \cH$. 
        Moreover, $\varphi''(t) = \psi_1(t) + \psi_2(t)$, where
        \begin{equation*}
        \psi_1(t) = \E_{x \sim \rho_t} \left[ \ps{w_t(x), H_V(x) w_t(x)}\right] \; \text{ and } \; \psi_2(t) = \E_{x \sim \rho_t} \left[ \|J w_t(x)\|_{HS}^2 \right].
        \end{equation*}
    The first term $\psi_1(t)$ is bounded using~\Cref{ass:V_Lipschitz}, $\psi_1(t) \leq M \|g\|_{L^2(\mu_n)}^2 \leq M B^2 I_{Stein}(\mu_n|\pi)$. The second term $\psi_2(t)$ is the most challenging to bound as $\|J w\|_{HS}$ cannot be controlled by $\|w\|$ for a general $w$. However, in our case, $w_t = -g \circ (\phi_t)^{-1}$, and $-J w_t \circ \phi_t = Jg (J \phi_t)^{-1}$. Therefore, $\|J w_t\circ \phi_t(x)\|_{HS}^2 \leq \|Jg(x)\|_{HS}^2 \|(J \phi_t)^{-1}(x)\|_{op}^2 \leq \alpha^2 B^2 I_{Stein}(\mu_n|\pi)$.
    %The second term , the first term is easily bounded by
    % $\ps{v,H_V v} \leq M \|v\|^2$. Then since $\rho_t = \phi_{t\#} \mu_n$ and $v_t = -g(\phi_t^{-1})$, by the transfer lemma $\mathbb{E}_{x \sim \rho_t}[\| v_t(x)\|^2]=\mathbb{E}_{x\sim \mu_n}[\|g(x)\|^2]\le B^2I_{Stein}(\mu_n|\pi)$, hence 
    %$
    %\E_{x\sim \rho_t}[ \ps{v_t(x), H_V(x) v_t(x)}]\le M B^2 I_{Stein}(\mu_n|\pi)$.
    %  The second term is the most challenging to bound.  
    %  By the chain rule 
    %  %for any $x$ we have $-J v_t(x)=Jg(\phi_t^{-1}(x))(J\phi_t)^{-1}(\phi_t^{-1}(x))$; hence by 
    %  and the transfer lemma, we first have
    %   $\E_{x \sim \rho_t} \left[\|J v_t(x)\|_{HS}^2\right]= \E_{x \sim \mu_n} \left[\|Jg(x) (J\phi_t)^{-1}(x)\|_{HS}^2\right]$. Then,  for any $x$, $ \|Jg(x) (J\phi_t)^{-1}(x)\|_{HS}^2 \le B^2I_{Stein}(\mu_n|\pi) \| (J\phi_t)^{-1}(x)\|_{op}^2$. On the other hand, $J\phi_t=I-tJg$, and if $t<\frac{1}{B\sqrt{C}}$ then $t\|Jg(x)\|<1$ and it can be shown that $\|(I - t Jg(x))^{-1}\| \le \alpha$ for $\gamma$ chosen as in \Cref{prop:descent}. Therefore $\|Jg(x) (J\phi_t)^{-1}(x)\|_{HS}^2\le \alpha^2 B^2 I_{Stein}(\mu_n|\pi)$ and $\varphi''(t)\le (\alpha^2+M)B^2I_{Stein}(\mu_n|\pi)$. 
    Combining each of the quantity in the Taylor expansion gives the desired result.
        %Sufficiently, we shall bound the term $\|Jg)(x) J(\phi_t)^{-1}(x)\|_{HS}^2$. 
    \end{proof}
Although the Hessian of $\KL(.|\pi)$ is not bounded over the whole tangent space, our proof relies on controlling the Hessian when restricted to $\cH$. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of \Cref{prop:descent}}\label{sec:proof_descent}

%We justify each step of the sketch of the proof of \Cref{sec:descent_lemma_Rd}. For notation purposes we will omit the injection $\iota$ when taking the $L^2$ scalar product with an element of $\cH$.

Let $g^{\prime}\in \cH$, $C = \left\|g^{\prime}\right\|_{\cH}^2$ and denote $g = \iota g^{\prime} \in L^2$,  for every $t \in [0,\gamma], \gamma\leq \frac{\alpha-1}{\alpha B C^{\frac{1}{2}}}$,  $\phi_t = (I - t  g)$. Denote $\rho_t = \phi_{t\#} \mu_n$. Then, $\rho_0 = \mu_n$ and $\rho_{\gamma} = \mu_{n+1}$. 
%Consider $n \geq 0$ fixed and $\gamma \leq \frac{\alpha-1}{\alpha BC^{\frac{1}{2}} }$. Denote $g = P_{\mu_n}\nabla \log\left(\frac{\mu_n}{\pi}\right)$ and 



\begin{lemma}\label{lem:inequalities}
	Suppose \Cref{ass:k_bounded} holds, i.e. the kernel and its gradient are bounded by some positive constant $B$. Then for any $x\in \X$:
	\begin{align}
&	\|g(x)\| \leq B C^{\frac{1}{2}}\\
&	\Vert J g(x)\Vert_{HS}\leq B C^{\frac{1}{2}} 
	\end{align}
	
\end{lemma}
\begin{proof}
	This is a consequence of the reproducing property and Cauchy-Schwarz inequality in the RKHS space. We know for any $x\in \X$, $g(x)=g'(x)$ and:
	\begin{equation*}
	\|g(x)\|^2=\sum_{i=1}^d \ps{k(x,.),g'_i}^2_{\cH_0}\le \|k(x,.)\|_{\cH_0}^2 \|g'\|_{\cH}^2\le B^2 C.
	\end{equation*}
	Similarly:
	\begin{align*}
	\|Jg(x)\|_{HS}^2&=\sum_{i,j=1}^d \left|\frac{\partial g_i(x)}{\partial x_j} \right|^2=\sum_{i,j=1}^d \ps{\partial_{x_j}k(x,.), g'_i}_{\cH_0}\le \sum_{i,j=1}^d \| \partial_{x_j}k(x,.)\|^2_{\cH_0} \|g'_i\|_{\cH_0}^2\\
	&=\| \nabla k(x,.)\|^2_{\cH}\|g'\|^2_{\cH}\le B^2 C.
	\end{align*}
\end{proof}




\begin{lemma}\label{lem:diffeo}
	Suppose that~\Cref{ass:k_bounded} hold. Then, for any $x\in \X$, $\|t Jg(x)\|_{op} \leq t B\sqrt{C}$ and for every $t < \frac{1}{B\sqrt{C}}$, $\phi_t$ is a diffeomorphism. Moreover, $\|(J\phi_t(x))^{-1}\|_{op} \leq \alpha$. 
	%$\phi_t = I-t\gamma P_{\mu_n}\nabla \log(\frac{\mu}{\pi})(x)$ for $t\in [0,1]$. Fix $\alpha >1$,  Then for $\gamma< \frac{\alpha-1}{\alpha BC^{\frac{1}{2}}} $, $\phi_t$ is a diffeomorphism.
\end{lemma}
\begin{proof}
	First, by \Cref{lem:inequalities}  we have $\|J g(x)\|_{op} \leq \|J g(x)\|_{HS} \leq B\sqrt{C}$.
	If $t < \frac{1}{B\sqrt{C}}$, then $\|t Jg(x)\|_{op} < 1$. Therefore, $J(\phi_t)(x) = I - t Jg(x)$ is regular for every $x$ and $\phi_t$ is a diffeomorphism. Moreover, 
	\begin{equation}
		\|(J\phi_t(x))^{-1}\|_{op} \leq \sum_{k=0}^\infty \|t Jg(x)\|_{op}^k \leq \sum_{k=0}^\infty \|t Jg(x)\|_{HS}^k \leq \sum_{k=0}^\infty (t B \sqrt{C})^k \leq \alpha,
	\end{equation}
	where we used $\gamma \leq \frac{\alpha-1}{\alpha BC^{\frac{1}{2}} }$.
\end{proof}
    %From \Cref{lem:velocity_field}, 
    % Using~\cite[Theorem 5.34]{villani2003topics}, 
    % the velocity field of $\rho_t$ is $v_t \in L^2(\rho_t)$ defined by $v_t(x) = -g(\phi_t^{-1}(x))$. In particular, $v_0 = -g$.

Denote $\varphi(t) = \KL(\rho_t|\pi)$. Using Taylor expansion,
\begin{equation}
    \label{eq:Taylor-integral-remainder-appendix}
    \varphi(\gamma) = \varphi(0) + \gamma \varphi'(0) + \int_{0}^{\gamma} (\gamma - t)\varphi''(t)dt.
\end{equation}
We now identify each term. First, $\varphi(0) = \KL(\mu_n|\pi)$ and $\varphi(\gamma) = \KL(\mu_{n+1}|\pi)$.


To compute $\varphi'(t)$ and $\varphi''(t)$ we have two options. Either we check the assumptions of the optimal transport theorems allowing to apply the chain rule~\cite{villani2003topics,ambrosio2008gradient}, or we do a direct computation. The latter is preferred, although differential calculus over the Wasserstein space is a powerful way to guess the formulas.

\begin{lemma} Denote $w_t(x) = -g(\phi_t^{-1}(x))$. Then,
	\begin{equation*}
		\varphi'(0)=\ps{\nabla_{W_2} \KL(\rho_0|\pi), w_0}_{L^2(\mu_n)} = -\langle S_{\mu_n}\nabla \log(\frac{\mu_n}{\pi}),g^{\prime}\rangle_{\cH},
	\end{equation*}
	and,
	\begin{equation*}
		\varphi''(t) = \ps{ w_t,Hess_{\KL(.|\pi)}(\rho_t) w_t}_{L^2(\rho_t)} = \int \left[\|Jg(x)(J\phi_t(x))^{-1}\|_{HS}^2 + \ps{g(x), H_V (\phi_t(x)) g(x)}\right] \mu_n(x)dx.
	\end{equation*}
\end{lemma}
\begin{proof}
We know by \Cref{lem:diffeo} that $\phi_t$ is a diffeomorphism, therefore, $\rho_t$ admits a density given by the change of variables formula:
	\begin{align}
	\rho_t(x) = \vert J \phi_t(\phi_t^{-1}(x))  \vert^{-1} \mu_n(\phi_{t}^{-1}(x)). 
	\end{align}
	Using the transfer lemma with $\rho_t=\phi_{t\#}\mu_n$, $\varphi(t)$ is given by:
	\begin{align*}
	\varphi(t) &= \int \log \left( \frac{\rho_t(y)}{\pi(y)}\right) \rho_t(y)dy\\
	&=  \int \log\left( \frac{\mu_n(x) \vert  J\phi_t(x) \vert^{-1}}{\pi(\phi_t(x))}\right) \mu_n(x)dx.
	\end{align*}
	We can now take the time derivative of $\varphi(t)$ which gives:
	\begin{equation*}
	\varphi'(t) = - \int tr\left(J\phi_t(x)^{-1} \frac{dJ\phi_t(x)}{dt}\right)\mu_n(x)dx - \int \ps{\nabla \log \pi(\phi_t(x)),\frac{d\phi_t(x)}{dt}} \mu_n(x)dx.  
	\end{equation*}
	Hence, we can use the explicit expression of $\phi_t$ to write:
	\begin{equation*}
		\varphi'(t) = \int tr(J\phi_t(x)^{-1} Jg(x))\mu_n(x)dx + \int \ps{\nabla \log \pi(\phi_t(x)), g(x)  }\mu_n(x)dx.  
	\end{equation*}
	The Jacobian at time $t=0$ is simply equal to the identity since $\phi_0=I$. It follows that $tr(J\phi_0(x)^{-1} Jg(x)) =tr(Jg(x))= div(g)(x)$ by definition of the divergence operator. Using an integration by parts:
	\begin{align*}
		\varphi'(0) &= -\int \left[-div( g )(x)- \ps{\nabla \log \pi(x),g(x)}\right]\mu_n(x)dx\\
	&= -\int\ps{ \nabla \log\left(\frac{\mu_n}{\pi}\right)(x), g(x)}\mu_n(x)dx
	= -\langle S_{\mu_n}\nabla \log(\frac{\mu_n}{\pi}),g^{\prime}\rangle_{\cH}.
	\end{align*}
	Now, we prove the second statement. First, 
	\begin{align*}
	\varphi''(t)= \int \left[tr((Jg(x)(J\phi_t(x))^{-1})^2) + \ps{g(x), H_V (\phi_t(x)) g(x)}\right] \mu_n(x)dx. 
	\end{align*}
	Since $Jg(x)$ and $J\phi_t(x)$ commutes, $tr((Jg(x)(J\phi_t(x))^{-1})^2) = \|Jg(x)(J\phi_t(x))^{-1}\|_{HS}^2$. Moreover, using the chain rule, 
	\begin{equation}
		-J w_t(x) = J (g \circ \phi_t^{-1})(x) = Jg(\phi_t^{-1}(x)) J(\phi_t^{-1})(x) = Jg(\phi_t^{-1}(x)) (J\phi_t)^{-1}(\phi_t^{-1}(x)).
	\end{equation}
		Therefore, $\|Jg(x)(J\phi_t(x))^{-1}\|_{HS}^2 = \|J w_t(\phi_t(x))\|_{HS}^2$, which proves the second part of the second statement. Using the transfer lemma,
		\begin{align*}
			\varphi''(t)&= \int \left[\|J w_t(y)\|_{HS}^2 +\ps{ w_t(y),H_V (y) w_t(y)}\right] \rho_t(y)dy\\
			&=\ps{ w_t,Hess_{\KL(.|\pi)}(\rho_t) w_t}_{L^2(\rho_t)},
			\end{align*}
			which concludes the proof.
\end{proof}
Denote
\begin{equation*}
        \psi_1(t) = \int \left[\|Jg(x)(J\phi_t(x))^{-1}\|_{HS}^2\right] \mu_n(x)dx \; \text{ and } \; \psi_2(t) = \int \ps{g(x), H_V (\phi_t(x)) g(x)}\mu_n(x)dx.
		\end{equation*}
	Then, $\varphi''(t) = \psi_1(t) + \psi_2(t)$.
% Second, using the chain rule in the Wasserstein space, 
% \begin{multline*}
% \varphi'(0) = \ps{\nabla_{W_2} \KL(\rho_0|\pi),v_0}_{L^2(\rho_0)} = \ps{P_{\rho_0}\nabla_{W_2} \KL(\rho_0|\pi),v_0}_{\cH}
%  = -\|g\|_{\cH}^2=-I_{Stein}(\mu_n|\pi)
% \end{multline*}
%  using $v_0 \in \cH$. 
% Finally, recall that the Hessian of $\KL(.|\pi)$ at $\mu$ is the operator $H_{\KL(.|\pi)}(\mu) : L^2(\mu) \to L^2(\mu)$ defined by 
% \begin{equation}
%     \ps{v, H_{\KL(.|\pi)}(\mu) v}_{\mu} = E_{X \sim \mu} \left[\|J v(X)\|_{HS}^2 + \ps{v(X), H_V(X) v(X)}\right],
% \end{equation}
% where $H_V$ is the Hessian matrix of $V = -\log(\pi)$ and $Jv$ the Jacobian matrix of $v \in L^2(\mu)$. Hence,
% \begin{equation}
%     \varphi''(t) %= \ps{v(t),H_{\cF}(\mu(t)) v(t)}_{\mu(t)} 
%     = E_{x_t \sim \rho_t} \left[ \|J v_t(x_t)\|_{HS}^2 + \ps{v_t(x_t), H_V(x_t) v_t(x_t)}\right].
% \end{equation}
We bound $\psi_1$ and $\psi_2$ separately. First, since the potential $V$ is $M$-smooth, 
\begin{align*}
	\psi_2(t) \leq M\int \|g(x)\|^2 \mu_n(x)dx \leq M B^2 C,
\end{align*}
by using \Cref{lem:inequalities}.
%$\ps{v,H_V v} \leq M \|v\|^2$. Therefore, 
% \begin{align*}
%     E_{x_t\sim \rho_t} \left[ \ps{ v_t(x_t), H_V(x_t) v_t(x_t) } \right] &= E_{x_0\sim \rho_0} \left[ \ps{v_t(\phi_t(x_0)), H_V(\phi_t(x_0)) v_t(\phi_t(x_0))}\right] \\
%     &= E_{x_0\sim \rho_0} \left[\ps{v_0, H_V(\phi_t(x_0)) v_0}\right]\\
%     &\leq M \|g\|_{L^2(\mu_n)}^2 \le M B^2 I_{Stein}(\mu_n|\pi).
% \end{align*}
%by domination of the RKHS norm over the $L^2$ norm.\aknote{add this in background}
% For the second term, note that by the chain rule :
% \begin{equation}
% -J v_t(x) = J (g \circ \phi_t^{-1})(x) = Jg(\phi_t^{-1}(x)) J(\phi_t^{-1})(x) = Jg(\phi_t^{-1}(x)) (J\phi_t)^{-1}(\phi_t^{-1}(x)),
% \end{equation}
% therefore $E_{x_t \sim \mu_t} \left(\|J v_t(x_t)\|_{HS}^2\right) = E_{x_0 \sim \rho_0} \left(\|Jg(x_0) J(\phi_t)^{-1}(x_0)\|_{HS}^2\right)$. Sufficiently, we shall bound the term $\|J(g)(x) J(\phi_t)^{-1}(x)\|_{HS}^2$. First,
Now, we bound $\psi_1(t)$ using \Cref{lem:diffeo} and \ref{lem:inequalities}:
\begin{equation}
    \|Jg(x)(J\phi_t(x))^{-1}\|_{HS}^2 \leq \|Jg(x)\|_{HS}^2 \| (J\phi_t(x))^{-1}\|_{op}^2 \leq \alpha^2 B^2 C.
\end{equation}


% Then, $J\phi_t(x) = I - t Jg(x)$ and, since $t < \frac{1}{B\sqrt{C}}$, we have $t\|Jg(x)\| < 1$. Therefore, 
% \begin{equation}
% \|(I - t Jg(x))^{-1}\|_{op} \leq \sum_{k=0}^{+\infty} \|t Jg(x)\|_{op}^k = \frac{1}{1-t\|Jg(x)\|_{op}} \leq \frac{1}{1-t\|Jg(x)\|_{HS}} \leq \frac{1}{1-\gamma B\sqrt{C}}.
% \end{equation}
% Therefore, 
% \begin{equation}
%     \|Jg(x) (J\phi_t)^{-1}(x)\|_{HS}^2 \leq B^2 I_{Stein}(\mu_n|\pi) \frac{1}{(1-\gamma B\sqrt{C})^2}.
% \end{equation}
% Hence, if $\alpha > 1$ and $\gamma \leq \frac{\alpha - 1}{\alpha B \sqrt{C}}$, then $\frac{1}{1-\gamma B\sqrt{C}} \leq \alpha$ and 
% \begin{equation}
%     \|Jg(x) J(\phi_t)^{-1}(x)\|_{HS}^2 \leq \alpha^2 B^2 I_{Stein}(\mu_n|\pi).
% \end{equation}
Finally, $\varphi''(t) \leq (\alpha^2 + M)B^2 C$. Plugging into~\eqref{eq:Taylor-integral-remainder-appendix} gives the result.


\begin{lemma}\label{lem:bounded_stein}
	Suppose \Cref{ass:k_bounded} holds, i.e. the kernel and its gradient are bounded by some positive constant $B$. Moreover,  assume that $\nabla \log(\pi)$ is $M$-Lipschitz  and that $\int \Vert x \Vert \mu_n(x)dx $ is uniformly bounded on $n$.	Then $I_{Stein}(\mu_n|\pi)$ remains bounded by some $C>0$, i.e. \Cref{ass:bounded_I_Stein} holds.%\aknote{this also holds under the stronger \Cref{ass:V_bounded}}
\end{lemma}
\begin{proof}
	For any $\mu$, we have :
	\begin{equation*}
	I_{Stein}(\mu|\pi)=\ps{\int \nabla \log \pi(x)k(x,.) + \nabla_1 k(x,.)d\mu(x), \int \nabla \log \pi(y)k(y,.) + \nabla_1 k(y,.)d\mu(y)}_{\cH}
	\end{equation*}
	Using the reproducing property and integration by parts it is possible to write $I_{Stein}(\mu|\pi)$ as:
	\begin{align*}
	I_{Stein}(\mu|\pi) =& \int \nabla_1.\nabla_2 k(x,y)d\mu(x)d\mu(y)\\ &+ \int \ps{\nabla \log \pi(y),\nabla_1 k(x,y)}d\mu(x)d\mu(y)+\int \ps{\nabla \log \pi(x),\nabla_1 k(y,x)}d\mu(x)d\mu(y)\\ 
	&+ \int \ps{\nabla \log \pi(x),\nabla \log \pi(y)} k(x,y)d\mu(x)d\mu(y).
	\end{align*}
	The terms involving the kernel are easily bounded since the kernel is bounded  with bounded derivatives.
		Using that $\nabla \log \pi$ is $M$-Lipschitz, it is easy to see that
		\begin{align}
		\Vert \nabla \log \pi(x) \Vert \leq \Vert \nabla \log \pi(0) \Vert + M \Vert x\Vert.	
		\end{align}
		Using the above inequality, one can directly conclude  that  $\int \Vert x \Vert \mu_n(x)dx $ remains bounded.
\end{proof}






\appendix


\section{Proofs}

In this section we prove our main results (Theorems~\ref{th:svgd} and~\ref{th:vrsgsgd}).

\subsection{Notations}

%%%%NOTATIONS TO DEFINE IN APPENDIX OR MAIN PAPER:
%\cF
%Definition in appendix vs deifnition in main paper
%Define: I, \X, \cH, \cF, P_{\mu_n}\nabla \log\left(\frac{\mu_n}{\pi}\right) = h_{\mu_n} tjrs bien defini, Wass grad under the metric of H cf my personal view of SVGD.
%Jacobian, H_0, HS, op, several inner products, Wasserstein gradients,/Hessian
%Feature map \Phi(x)
%W_1, W_2, Dirac

\subsection{Fundamental inequality}

We start by stating a fundamental inequality satisfied by $\cF$ for any update of the form
\begin{equation}
    \mu_{n+1} = \left(I - \gamma g\right) \# \mu_n,
\end{equation}
where $g \in \cH$.
\begin{proposition}
\label{prop:TL}
Let Assumptions~\ref{ass:k_bounded} and~\ref{ass:V_Lipschitz} hold true. Let $\alpha > 1$ and choose $\gamma > 0$ such that $\gamma \|g\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$. Then,
    \begin{equation}
    \label{eq:TL}
        \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \gamma\ps{h_{\mu_n},g}_{\cH} +  \frac{\gamma^2 K}{2}\|g\|_{\cH}^2,
    \end{equation}
    where $K = (\alpha^2 + M)B$.
\end{proposition}
Inequality~\eqref{eq:TL} plays the role of a Taylor inequality, where $h_{\mu_n}$ is the Wasserstein gradient of $\cF$ at $\mu_n$ under the metric induced by $\cH$. Proposition~\ref{prop:TL} generalizes \cite[Proposition 5]{korba2020non}. Indeed, \cite[Proposition 5]{korba2020non} can be obtained from our Proposition~\ref{prop:TL} by taking $g = h_{\mu_n}$, by assuming the uniform upper bound $\|h_{\mu_n}\|_{\cH}^2 \leq C$ (see \cite[Assumption A3]{korba2020non}) and by restricting the step size to $\gamma C^{\frac{1}{2}} \leq \frac{\alpha-1}{\alpha B}$. Note that assuming $\gamma C^{\frac{1}{2}} \leq \frac{\alpha-1}{\alpha B}$ is more restrictive than assuming $\gamma \|h_{\mu_n}\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$. 

Yet, the proof of our Proposition~\ref{prop:TL} is similar to the proof of \cite[Proposition 5]{korba2020non}, by replacing their $h_{\mu_n}$ by our $g$ and their uniform upper bound $C$ by our $\|g\|_{\cH}^2$. Therefore, we only sketch the main arguments, and further details can be found in \cite[Section 11.2]{korba2020non}
\begin{proof}
    Let $\phi_t = I - t g$ for $t \in [0,\gamma]$ and $\rho_t = (\phi_t) \# \mu_n$. Note that $\rho_0 = \mu_n$ and $\rho_{\gamma} = \mu_{n+1}$. 
    First, for every $x \in \X$,
    \begin{equation}
    \label{eq:gbound}
	\|g(x)\|^2=\sum_{i=1}^d \ps{k(x,.),g_i}^2_{\cH_0} \le \|k(x,.)\|_{\cH_0}^2 \|g\|_{\cH}^2\le B^2 \|g\|_{\cH}^2,
	\end{equation}
	and,
	\begin{align}
	\label{eq:Jgbound}
	\|Jg(x)\|_{\HS}^2&=\sum_{i,j=1}^d \left|\frac{\partial g_i(x)}{\partial x_j} \right|^2=\sum_{i,j=1}^d \ps{\partial_{x_j}k(x,.), g_i}_{\cH_0}^2\le \sum_{i,j=1}^d \| \partial_{x_j}k(x,.)\|^2_{\cH_0} \|g_i\|_{\cH_0}^2 \nonumber\\
	&=\| \nabla k(x,.)\|^2_{\cH}\|g\|^2_{\cH}\le B^2 \|g\|^2_{\cH}.
	\end{align}
    Hence, 
    \begin{equation}
    \label{eq:invers-glob}
        \|t Jg(x)\|_{\op} \leq \|t Jg(x)\|_{\HS} \leq \gamma B \|g\|_{\cH} \leq \frac{\alpha-1}{\alpha} < 1,
    \end{equation} 
    using our assumption on the step size $\gamma$. Inequality~\eqref{eq:invers-glob} proves that $\phi_t$ is a diffeomorphism for every $t \in [0,\gamma]$. Moreover, 
    \begin{equation}
    \label{eq:alpha}
		\|(J\phi_t(x))^{-1}\|_{\op} \leq \sum_{k=0}^\infty \|t Jg(x)\|_{\op}^k \leq \sum_{k=0}^\infty \left(\frac{\alpha-1}{\alpha}\right)^k = \alpha.
    \end{equation}
    Using~\cite[Theorem 5.34]{villani2003topics}, the velocity field ruling the time evolution of $\rho_t$ is $w_t \in L^2(\rho_t)$ defined by $w_t(x) = -g(\phi_t^{-1}(x))$. %Note that $w_0 = -g\in \cH$.    
    
    Denote $\varphi(t) = \cF(\rho_t)$. Using a Taylor expansion,
        \begin{equation}
        \label{eq:Taylor}
            \varphi(\gamma) = \varphi(0) + \gamma \varphi'(0) + \int_{0}^{\gamma} (\gamma - t)\varphi''(t)dt.
        \end{equation}
        We now identify each term. First, 
        \begin{equation*}
    \varphi(0) = \cF(\mu_n)\; \text{ and } \;\varphi(\gamma) = \cF(\mu_{n+1}).
        \end{equation*} 
        Then, using the chain rule~\cite[Section 8.2]{villani2003topics}, 
        \begin{equation*}
            \varphi'(t)=\ps{\nabla_{W_2} \cF(\rho_t),w_t}_{L^2(\rho_t)}\; \text{ and } \;\varphi''(t) = \ps{w_t,H_{W_2}{\cF}(\rho_t)w_t}_{L^2(\rho_t)}.
        \end{equation*}
        Therefore, using $g \in \cH$,
        \begin{equation*}
            \varphi'(0) = -\ps{\nabla_{W_2} \cF(\mu_n),g}_{L^2(\mu_n)} = -\ps{h_{\mu_n},g}_{\cH}.
        \end{equation*}
        Moreover, $\varphi''(t) = \psi_1(t) + \psi_2(t)$, where
        \begin{equation*}
        \psi_1(t) = \E_{x \sim \rho_t} \left[ \ps{w_t(x), H_V(x) w_t(x)}\right] \; \text{ and } \; \psi_2(t) = \E_{x \sim \rho_t} \left[ \|J w_t(x)\|_{\HS}^2 \right].
        \end{equation*}
        Recall that $w_t = -g \circ (\phi_t)^{-1}$.
    The first term $\psi_1(t)$ is bounded using the transfer lemma, \Cref{ass:V_Lipschitz} and Inequality~\eqref{eq:gbound}:
    \begin{equation*}
        \psi_1(t) = \E_{x \sim \mu_n} \left[ \ps{g(x), H_V(\phi_t(x)) g(x)}\right] \leq M \|g\|_{L^2(\mu_n)}^2 \leq M B^2 \|g\|_{\cH}^2.
    \end{equation*} 
    For the second term $\psi_2(t)$, using the chain rule, $-J w_t \circ \phi_t = Jg (J \phi_t)^{-1}$. Therefore, 
    \begin{equation*}
        \|J w_t\circ \phi_t(x)\|_{\HS}^2 \leq \|Jg(x)\|_{\HS}^2 \|(J \phi_t)^{-1}(x)\|_{\op}^2 \leq \alpha^2 B^2 \|g\|_{\cH}^2,
    \end{equation*}
    using \eqref{eq:Jgbound} and \eqref{eq:alpha}.
    %The second term , the first term is easily bounded by
    % $\ps{v,H_V v} \leq M \|v\|^2$. Then since $\rho_t = \phi_{t\#} \mu_n$ and $v_t = -g(\phi_t^{-1})$, by the transfer lemma $\mathbb{E}_{x \sim \rho_t}[\| v_t(x)\|^2]=\mathbb{E}_{x\sim \mu_n}[\|g(x)\|^2]\le B^2I_{Stein}(\mu_n|\pi)$, hence 
    %$
    %\E_{x\sim \rho_t}[ \ps{v_t(x), H_V(x) v_t(x)}]\le M B^2 I_{Stein}(\mu_n|\pi)$.
    %  The second term is the most challenging to bound.  
    %  By the chain rule 
    %  %for any $x$ we have $-J v_t(x)=Jg(\phi_t^{-1}(x))(J\phi_t)^{-1}(\phi_t^{-1}(x))$; hence by 
    %  and the transfer lemma, we first have
    %   $\E_{x \sim \rho_t} \left[\|J v_t(x)\|_{HS}^2\right]= \E_{x \sim \mu_n} \left[\|Jg(x) (J\phi_t)^{-1}(x)\|_{HS}^2\right]$. Then,  for any $x$, $ \|Jg(x) (J\phi_t)^{-1}(x)\|_{HS}^2 \le B^2I_{Stein}(\mu_n|\pi) \| (J\phi_t)^{-1}(x)\|_{op}^2$. On the other hand, $J\phi_t=I-tJg$, and if $t<\frac{1}{B\sqrt{C}}$ then $t\|Jg(x)\|<1$ and it can be shown that $\|(I - t Jg(x))^{-1}\| \le \alpha$ for $\gamma$ chosen as in \Cref{prop:descent}. Therefore $\|Jg(x) (J\phi_t)^{-1}(x)\|_{HS}^2\le \alpha^2 B^2 I_{Stein}(\mu_n|\pi)$ and $\varphi''(t)\le (\alpha^2+M)B^2I_{Stein}(\mu_n|\pi)$. 
    Combining each of the quantity in the Taylor expansion~\eqref{eq:Taylor} gives the desired result.
        %Sufficiently, we shall bound the term $\|Jg)(x) J(\phi_t)^{-1}(x)\|_{HS}^2$. 

\end{proof}
\subsection{Proof of Theorem~\ref{th:SVGD}}
As mentioned in the last section, Proposition~\ref{prop:TL} can be applied to SVGD, \textit{i.e.}, the update 
\begin{equation*}
    \mu_{n+1} = (I - \gamma h_{\mu_n}) \# \mu_n,
\end{equation*}
by setting $g = h_{\mu_n} \in \cH$. In this case, we obtain the following.
\begin{lemma}
\label{lem:svgd}
Let Assumptions~\ref{ass:k_bounded} and~\ref{ass:V_Lipschitz} hold true. Let $\alpha > 1$ and choose $\gamma > 0$ such that $\gamma \|h_{\mu_n}\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$. Then,
    \begin{equation}
    \label{eq:TL-svgd}
        \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \gamma\left(1 - \frac{\gamma K}{2}\right)\|h_{\mu_n}\|_{\cH}^2,
    \end{equation}
    where $K = (\alpha^2 + M)B$.
\end{lemma}

\begin{lemma}
\label{lem:T1}
     Let Assumptions \ref{ass:k_bounded}, \ref{ass:V_Lipschitz} and \ref{ass:T1} hold true. Then, for every $\mu \in \cP_2(\X)$,\asnote{$\cP_2$ ?}
\begin{equation*}
    \|h_{\mu}\|_{\cH} \leq B \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x)\right) + BM \lambda \sqrt{\cF(\mu)}.
\end{equation*}
\end{lemma}
\begin{proof}
Using \Cref{ass:k_bounded}
    \begin{align*}
        \|h_{\mu}\|_{\cH} &= \left\|\E_{x \sim \mu} \left(\nabla F(x)\Phi(x) - \nabla \Phi(x)\right) \right\|_{\cH} \\
        &\leq \E_{x \sim \mu} \left\| \nabla F(x)\Phi(x) - \nabla \Phi(x) \right\|_{\cH}\\
        &\leq \E_{x \sim \mu} \left\| \nabla F(x)\Phi(x)\right\|_{\cH} + \E_{x \sim \mu} \left\|\nabla \Phi(x) \right\|_{\cH}\\
        &= \E_{x \sim \mu} \left\|\nabla F(x)\right\|\left\|\Phi(x)\right\|_{\cH} + \E_{x \sim \mu} \left\|\nabla \Phi(x) \right\|_{\cH}\\
        &\leq B \left( \E_{x \sim \mu} \left\|\nabla F(x)\right\| + 1 \right).
    \end{align*}
Using \Cref{ass:V_Lipschitz}, $\|\nabla F(x)\| \leq \|\nabla F(0)\| + M\|x\|$. Therefore, using the triangle inequality for the metric $W_1$,
    \begin{align*}
        \|h_{\mu}\|_{\cH} &\leq B \left(1 + \|\nabla F(0)\| + M\int \|x\|d\mu(x) \right)\\
        &= B \left(1 + \|\nabla F(0)\| + M W_1(\mu,\delta_0) \right)\\
        &\leq B \left(1 + \|\nabla F(0)\| + M W_1(\pi,\delta_0) \right) + BM W_1(\mu,\pi).
    \end{align*}
We conclude using Talagrand's inequality~\eqref{eq:T1}: $W_1(\mu,\pi) \leq \lambda \sqrt{\cF(\mu)}$.
\end{proof}
\begin{theorem}
  Let Assumptions \ref{ass:k_bounded}, \ref{ass:V_Lipschitz} and \ref{ass:T1} hold true. Let $\alpha > 1$ and choose $\gamma > 0$ such that 
  \begin{equation}
  \label{eq:condition-step}
      \gamma \left(B \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x)\right) + BM \lambda \sqrt{\cF(\mu_0)} \right) \leq \frac{\alpha-1}{\alpha B}.
  \end{equation}
  Then, 
  \begin{equation}
    \label{eq:TL-svgd-cst}
        \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \gamma\left(1 - \frac{\gamma K}{2}\right)\|h_{\mu_n}\|_{\cH}^2.
    \end{equation}
\end{theorem}

    We now prove Theorem~\ref{th:svgd} by induction. First, if $\gamma > 0$ satisfies \eqref{eq:condition-step}, then, using Lemma~\ref{lem:T1}, $\gamma \|h_{\mu_0}\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$. Therefore, using Lemma~\ref{lem:svgd}, 
    \begin{equation*}
        \cF(\mu_{1}) \leq \cF(\mu_{0}) - \gamma\left(1 - \frac{\gamma K}{2}\right)\|h_{\mu_0}\|_{\cH}^2,
    \end{equation*}
\textit{i.e.}, Inequality~\eqref{eq:TL-svgd-cst} holds with $n = 0$. 

Now, assume that Inequality~\eqref{eq:TL-svgd-cst} holds for every $k \in \{0,\ldots,n-1\}$. Then, $\cF(\mu_n) \leq \cF(\mu_0)$ and
\begin{align*}
    &B \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x)\right) + BM \lambda \sqrt{\cF(\mu_n)}\\ \leq &B \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x)\right) + BM \lambda \sqrt{\cF(\mu_0)}. 
\end{align*}
Therefore, if $\gamma > 0$ satisfies \eqref{eq:condition-step}, then $\gamma \|h_{\mu_n}\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$. Indeed, using Lemma~\ref{lem:T1}
\begin{align*}
\gamma \|h_{\mu_n}\|_{\cH}
&\leq  \gamma \left(B \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x)\right) + BM \lambda \sqrt{\cF(\mu_n)} \right) \\
       &\leq \gamma \left(B \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x)\right) + BM \lambda \sqrt{\cF(\mu_0)} \right) \\
       &\leq \frac{\alpha-1}{\alpha B}.
  \end{align*}
  Therefore, using Lemma~\ref{lem:svgd}, Inequality~\eqref{eq:TL-svgd-cst} holds at step $n$:
  \begin{equation*}
        \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \gamma\left(1 - \frac{\gamma K}{2}\right)\|h_{\mu_n}\|_{\cH}^2.
    \end{equation*}
    Finally, it remains to recall that $\|h_{\mu_n}\|_{\cH}^2 = I_{\st}(\mu_n|\pi)$.
    
\subsection{Proof of Theorem~\ref{th:vrsvsgd}}
We now study VR-SVSGD, \textit{i.e.}, the update 
\begin{equation*}
    \mu_{n+1} = (I - \gamma g_n) \# \mu_n,
\end{equation*}
where $g_n$ is defined by





















\section{Appendix: Lukang's Draft}

\subsection{Setup}

\begin{equation}
F=\frac{1}{n} \sum_{i=1}^{n} f_{i}, \qquad \pi \propto e^{-F }
\end{equation}
\begin{equation}
		F_{n}=\frac{1}{|I_{n}|} \sum_{i \in I_n} f_{i},  \qquad \sigma_{n} \propto e^{-F_{n}}
  \end{equation}

	\begin{equation}
		g_n^{\prime}\in \cH, \qquad g_n = \iota g \in L^2
	\end{equation}
    \begin{equation}
		\phi_{t}(x)=\left(I-t g_n\right) (x), \qquad \mu_{t}=\left(\phi_{t}\right)_{\#} \mu_{n}
  \end{equation}
    
    \begin{equation}   
	\varphi(t)=\int \log \left(\frac{\mu_{t}}{\pi}\right) d \mu_{t}
\end{equation}
  From Lemma 4.  when $t \in [0,\gamma], \gamma\leq \frac{\alpha-1}{\alpha B \left\|g_n^{\prime}\right\|_{\cH}}$, there is
  {
  	\begin{equation}
  		\varphi(\gamma)\leq \varphi(0)-\gamma \left\langle S_{\mu_n}\nabla\log \frac{\mu_n}{\pi}, g_n^{\prime} \right\rangle_{\cH} +
  		\frac{(\alpha^2+M) \gamma^2}{2} \left\langle g_n^{\prime},g_n^{\prime} \right\rangle_{\cH}
  \end{equation}}
	
  \section{PAGE}

  We define the update rule as:
    
  \begin{equation}
  g_{n+1}=\begin{cases}
        P_{\mu_{n+1}}\nabla \log \left(\frac{\mu_{n+1}}{\pi}\right) & \qquad {p},\\
        g_n+P_{\mu_{n+1}} \nabla \log \left(\frac{\mu_{n+1}}{\sigma_{I^{\prime}}}\right)-P_{\mu_{n}} \nabla \log \left(\frac{\mu_{n}}{\sigma_{I^{\prime}}}\right) & \qquad {1-p}, 
        \end{cases}   
      \end{equation}
{where  $\sigma_{I^{\prime}} \sim \exp^{-f_{I^{\prime}}}$, $f_{I^{\prime}} = \frac{1}{b^{\prime}}\sum_{i\in I^{\prime}}f_i$.}
     

     This is the mean field limit of the following algorithm SVSGD with variance reduction. Indeed,
     the update rule in $n+1$ step SVSGD is 
     \begin{equation}
     	\begin{split}
     		&\quad \frac{1}{l} \sum_{j=1}^{l}\left[k\left(x_{j}^{n+1},\cdot\right)\nabla \log\left(\sigma_{n+1}\right)\left(x_{j}^{n+1}\right)+\nabla k\left(x_j^{n+1},\cdot\right)\right] \\
     		&= \frac{1}{l} \sum_{j=1}^{l}\left[k\left(x_{j}^{n+1},\cdot\right)\left(\nabla\log\left(\sigma_n\right)\left(x_j^n\right)+\nabla \log\left(\sigma_{I^{\prime}}\right)\left(x_j^{n+1}\right)-\nabla \log\left(\sigma_{I^{\prime}}\right)\left(x_j^n\right)\right)+\nabla k\left(x_j^{n+1},\cdot\right)\right] \\
     		&= \frac{1}{l}\sum_{j=1}^{l}\left[k\left(x_j^n,\cdot\right) \nabla \log\left(\sigma_n\right)\left(x_j^n\right)+\nabla k\left(x_j^n,\cdot\right)\right] +\frac{1}{l} \sum_{j=1}^{l}\left[ k\left(x_j^{n+1},\cdot\right) \nabla \log\left(\sigma_{I^{\prime}}\right)\left(x_j^{n+1}\right)+\nabla k\left(x_j^{n+1},\cdot\right)\right]\\
     		&-\frac{1}{l} \sum_{j=1}^{l}\left[ k\left(x_j^{n},\cdot\right) \nabla \log\left(\sigma_{I^{\prime}}\right)\left(x_j^{n}\right)+\nabla k\left(x_j^{n},\cdot\right)\right]+\frac{1}{l} \sum_{j=1}^{l}\left[\left (k\left(x_j^{n+1},\cdot\right)-k\left(x_j^n,\cdot\right)\right)\left(\nabla \log\left(\sigma_n\right)\left(x_j^n\right)-\nabla \log\left(\sigma_{I^{\prime}}\right)\left(x_j^n\right)\right)\right]\\
     		&\rightarrow P_{\mu_{n}} \nabla \log \left(\frac{\mu_{n}}{\sigma_{n}}\right)+P_{\mu_{n+1}} \nabla \log \left(\frac{\mu_{n+1}}{\sigma_{I^{\prime}}}\right)-P_{\mu_{n}} \nabla \log \left(\frac{\mu_{n}}{\sigma_{I^{\prime}}}\right)+\int\left(k(\phi_{\gamma}(x),\cdot)-k(x,\cdot)\right)\nabla \log \left(\frac{\sigma_n}{\sigma_{I^{\prime}}}\right)\left(x\right)d\mu_n\left(x\right)
     	\end{split}
     \end{equation}
     So by induction,
     \begin{equation}
     	g_n(x)=P_{\mu_n}\nabla\log(\frac{\mu_n}{\sigma_n})(x)-\sum_{i=1}^{n-1}\int\left(
	   	k(\phi_{\gamma}^{(i)}(x^{\prime}),x)-k(x^{\prime},x)\right)\nabla\log(\frac{\sigma_i}{\sigma_{I^{\prime}}^{i}})(x^{\prime})d\mu_i(x^{\prime}),
     \end{equation}
 where $\nabla\log(\sigma_n)$ follows the same update rule as in PAGE, $\phi_{t}^{(i)}(x)=\left(I-t g_i\right) (x)$.\adil{(so you see (16) doesn't increase the calculation of gradient, but comparing with mean field limit of traditional PAGE, it increases the calculation of sum, I think this is as fast as the mean field limit of traditional PAGE. )}


        	
\section{Main Proof}

	By similar calculation as in PAGE, we have,
	
	
\begin{equation}
	\begin{split} \varphi(\gamma) & \leqslant \varphi(0)-\frac{\gamma}{2}\left\|S_{\mu_{n}} \nabla \log \left(\frac{\mu_{n}}{\pi}\right)\right\|_{\cH}^{2}-\left(\frac{1}{2 \gamma}-\frac{\alpha^2+M}{2}\right)  \gamma^{2}\left\|g^{\prime}_n\right\|_{\cH}^{2} \\ &+\frac{\gamma}{2}\left\|S_{\mu_{n}} \nabla \log \left(\frac{\mu_{n}}{\pi}\right)-g^{\prime}_n\right\|_{\cH}^{2}
	\end{split}
\end{equation}

here 
\begin{equation}
	\gamma\leq \frac{\alpha-1}{\alpha B \left\|g^{\prime}_n\right\|_{\cH}}.
\end{equation}
      
      Then a direct calculation now reveals that
      
      \begin{equation}
      	\begin{split}
      		 &\quad \text{E}\left[\left\|g_{n+1}^{\prime}-S_{\mu_{n+1}}\nabla \log\left(\frac{\mu_{n+1}}{\pi}\right)\right\|_{\cH}^2\right] \\
      		 &= (1-p) \text{E}\left[\left\|\right.\right. g_{n+1}^{\prime} + S_{\mu_{n+1}}\nabla \log \left(\frac{\mu_{n+1}}{\sigma_{I^{\prime}}}\right)-S_{\mu_{n}}\nabla \log \left(\frac{\mu_{n}}{\sigma_{I^{\prime}}}\right)\left.\left.\right\|_{\cH}^2\right] \\ 
      		& \leq (1-p)\text{E}\left[ \left\|\right.\right. S_{\mu_{n}}\nabla \log \left(\frac{\mu_{n}}{\pi}\right)-g_n^{\prime}\left.\left.\right\|_{\cH}^{2}\right] \\
      		& + (1-p) \text{E}\left[\left\|\right.\right. S_{\mu_{n}}\nabla \log \left(\frac{1}{\pi}\right) + S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)-S_{\mu_{n}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)+ \\ & 
      		-S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\pi}\right) \left.\left.\right\|_{\cH}^{2}\right]\\
      		& \leq (1-p)\text{E}\left[ \left\|\right.\right. S_{\mu_{n}}\nabla \log \left(\frac{\mu_{n}}{\pi}\right)-g_n^{\prime}\left.\left.\right\|_{\cH}^{2}\right] \\
      		& + (1-p) \text{E}\left[\left\|\right.\right. S_{\mu_{n}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)
      		-S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right) \left.\left.\right\|_{\cH}^{2}\right],
      	\end{split}
      \end{equation}
using $\frac{1}{n}\sum_{i = 1}^n \|a_i - \bar{a}\|^2 \leq \frac{1}{n}\sum_{i = 1}^n \|a_i\|^2$, where $\bar{a} = \frac{1}{n}\sum_{i=1}^n a_i$. 
  \section{Key Inequality}
	
		

  \begin{theorem}
    
    If $\gamma_n \cO(\bE \Phi_n) \leq C$, then
    \begin{equation}
      \bE(\Phi_{n+1})\leq \bE(\Phi_n)-\frac{1}{2}\bE\gamma_n\left\|g_n^{\prime}\right\|_{\cH}^2,
    \end{equation}
  
  where $\Phi_n:=\KL(\mu_n|\pi)+\frac{\gamma_n}{2p}\left\|g_n^{\prime}-S_{\mu_n}\nabla\log\left(\frac{\mu_n}{\sigma_n}\right)\right\|_{\cH}^2$. (In the proof,we use $\gamma$ instead of $\gamma_n$ for simplicity.)
    
    
  \end{theorem}
  
  \begin{proof}
    We need to estimate
    \begin{equation}
      \left\|S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\pi}\right)-S_{\mu_{n}}\nabla \log \left(\frac{1}{\pi}\right)\right\|_{H}^{2},
    \end{equation}
    and
      \begin{equation}
    \left\|S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)-S_{\mu_{n}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)\right\|_{H}^{2}
      \end{equation}
  
  
    First,
    
 \begin{equation}
  \begin{split}
    &S_{\mu_{n+1}} \nabla \log \left(\frac{1}{\pi}\right)-S_{\mu_{n}}\nabla \log \left(\frac{1}{\pi}\right)\\
  &=-\int k(y,\cdot)\nabla\log(\pi(y))d\mu_{n+1}(y)+\int k(\phi_{\gamma}(x),\cdot)\nabla\log(\pi(x))d\mu_n(x)\\
  &-\int k(\phi_{\gamma}(x),\cdot)\nabla\log({\pi(x)})d\mu_n(x)+\int k(x,\cdot)\nabla\log({\pi(x)})d\mu_n(x)\\
  &=I + II .
  \end{split}
\end{equation}
  
We bound each term separately.
  
\begin{equation}
  \sqN{I}=\int\int k(\phi_{\gamma}(x),\phi_{\gamma}(x^{\prime}))(\nabla\log(x)-\nabla\log(\pi(\phi_{\gamma}(x))))(\nabla\log(x^{\prime})-\nabla\log(\pi(\phi_{\gamma}(x^{\prime}))))d\mu(x)d\mu(x^{\prime})
\end{equation}
  since F is $L$-smooth, so
  
\begin{equation}
  |\nabla\log(x^{\prime})-\nabla\log(\pi(\phi_{\gamma}(x^{\prime})))|
\leq L |x-\phi_{\gamma}(x)|
	= L\gamma|g_n\left(x\right)|
\leq L B \gamma\left\|g^{\prime}_n\right\|_{\cH},
\end{equation}
  then by Cauchy-Schwartz inequality and $k(x,x)$ is bounded by C, we derive
  
  \begin{equation}
  \sqN{I}\leq C L^2 B^2\gamma^2 \left\|g^{\prime}_n\right\|_{\cH}^{2}.
  \end{equation}
  Now,
\begin{equation}
	\begin{split}
		\sqN{II}=\int\int \left(k(\phi_{\gamma}\left(x\right),\phi_{\gamma}\left(x^{\prime}\right))-k(\phi_{\gamma}\left(x\right),x^{\prime})-k(x,\phi_{\gamma}\left(x^{\prime}\right))+k(x,x^{\prime})
		\right)\nabla\log \left(\pi\left(x\right)\right)\nabla\log \left(\pi\left(x^{\prime}\right)\right) d\mu_n(x)d\mu_n(x^{\prime})
	\end{split}
\end{equation}
 

  
  We need to know the order of $|k(\phi_{\gamma}(x),\phi_{\gamma}(x^{\prime}))-k(\phi_{\gamma}(x),x^{\prime})-k(x,\phi_{\gamma}(x^{\prime}))+k(x,x^{\prime})|$, in the following we assume supremum norm of the second order derivatives of $k$ are bounded by C, then by mean value theorem, we have
  
  \begin{equation}
  	\begin{split}
  		&\quad |k(\phi_{\gamma}(x),\phi_{\gamma}(x^{\prime}))-k(\phi_{\gamma}(x),x^{\prime})-k(x,\phi_{\gamma}(x^{\prime}))+k(x,x^{\prime})|\\
  		& \leq |\langle \phi_{\gamma}\left(x^{\prime}\right)-x^{\prime},\nabla_2 k\left(\phi_{\gamma}\left(x\right),\phi_{\theta_1}(x^{\prime})\right)\rangle-\langle \phi_{\gamma}\left(x^{\prime}\right)-x^{\prime},\nabla_2 k\left(x,\phi_{\theta_2}(x^{\prime})\right)\rangle|\\
  		&\leq
		C|\phi_{\gamma}(x)-x|(|\phi_{\gamma}(x)-x|+|\phi_{\theta_1}(x^{\prime})-\phi_{\theta_2}(x^{\prime})|) \\
		& \leq 2C B^2 \gamma^2 \left\|g_n^{\prime}\right\|_{\cH}^2
  	\end{split}
  \end{equation}


  Then, we know,
  \begin{equation}
  \sqN{II} \leq 2CB^2\left(\int|\nabla\log(\pi(x))|d\mu_n(x)\right)^2\gamma^2\left\|g_n^{\prime}\right\|_{\cH}^2
  \end{equation}

So
\begin{equation}
	\left\|S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\pi}\right)-S_{\mu_{n}}\nabla \log \left(\frac{1}{\pi}\right)\right\|_{H}^{2}\leq CB^2\left(L^2+\left(\int|\nabla\log(\pi(x))|d\mu_n(x)\right)^2\right)\gamma^2\left\|g_n^{\prime}\right\|_{\cH}^2 ,
\end{equation}
  
  similarly, we have 
  
  \begin{equation}
  	\left\|S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)-S_{\mu_{n}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)\right\|_{H}^{2}\leq CB^2\left(L^2+\left(\int|\nabla\log(\sigma_{I^{\prime}}(x))|d\mu_n(x)\right)^2\right)\gamma^2\left\|g_n^{\prime}\right\|_{\cH}^2 ,
  \end{equation}
  
  %\begin{equation}
  %	\begin{split}
  	%	& \quad \left\|\int\left(k(\phi_{\gamma}(x),\cdot)-k(x,\cdot)\right)\nabla \log \left(\frac{\sigma_n}{\sigma_{I^{\prime}}}\right)\left(x\right)d\mu_n\left(x\right)\right\|_H^2\\
  	%	&\!\!\!\!\!=\int\int\left(k\left(\phi_{\gamma}\left(x\right),\phi_{\gamma}\left(x^{\prime}\right)\right)-k\left(\phi_{\gamma}\left(x\right),x^{\prime}\right)-k\left(x,\phi_{\gamma}\left(x^{\prime}\right)\right)+k\left(x,x^{\prime}\right)
  	%	\right)\nabla\log \left(\frac{\sigma_{n}}{\sigma_{I^{\prime}}}\right)\left(x\right)\nabla\log \left(\frac{\sigma_n}{\sigma_{I^{\prime}}}\right)\left(x^{\prime}\right) d\mu_n(x)d\mu_n(x^{\prime})\\
  	%	&\!\!\!\!\!\leq C_3\left(\int |\nabla \log\left(\frac{\sigma_n}{\sigma_{I^{\prime}}}\right)|d\mu_n\right)^2\gamma^2\left\|S_{\mu_n}\nabla\log(\frac{\mu_n}{\sigma_n})\right\|_H^2
  	%\end{split}
  	%\end{equation}
  
  
  So if $\gamma$ is small enough, s.t 
  
  \begin{equation}
  	\begin{split}
  		&\quad\frac{\gamma}{2p}\left(CB^2\left(L^2+\left(\int|\nabla\log(\sigma_{I^{\prime}}(x))|d\mu_n(x)\right)^2\right)+CB^2\left(L^2+\left(\int|\nabla\log(\pi(x))|d\mu_n(x)\right)^2\right)\right)\\
  		& -\left(\frac{1}{2\gamma}-\frac{\alpha^2+M}{2}\right)\leq 0,
  	\end{split}
    \end{equation}
 by choosing \begin{equation}
 	\gamma^2\leq \frac{\frac{\alpha^2+M}{2}}{1+\frac{2CB^2L^2+CB^2\left(\left(\int|\nabla\log(\pi)(x)|d\mu_n(x)\right)^2+\left(\int|\nabla\log(\sigma_{I^{\prime}})(x)|d\mu_n(x)\right)^2\right)}{2p}},
 \end{equation} then last condition holds. 
 
  Then
  \begin{equation}
    \bE(\Phi_{n+1}) \leq \bE(\Phi_n)-\frac{1}{2}\bE (\gamma \left\|g_n^{\prime}\right\|_{\cH}^2)
  \end{equation}
  \end{proof}
If we let
\begin{equation}
	\bar{\gamma_n}^2=\frac{(\alpha-1)^2}{\alpha^2B^2\left\|g_n^{\prime}\right\|_{\cH}^2},
\end{equation}  
\begin{equation}
	\tilde{\gamma_n}^2= \frac{\frac{\alpha^2+M}{2}}{1+\frac{2CB^2L^2+CB^2\left(\left(\int|\nabla\log(\pi)(x)|d\mu_n(x)\right)^2+\left(\int|\nabla\log(\sigma_{I^{\prime}})(x)|d\mu_n(x)\right)^2\right)}{2p}},
\end{equation}
then $\gamma_n=\min\left(\bar{\gamma}_n,\tilde{\gamma_n}\right)$ satisfies (19) and (34). Now we need to bound the denominators in (36) and (37).

\begin{lemma}
	If there existing a constant $a>0$ and a point $x_0$ such that $\int e^{ad(x_0,x)^2}d\pi(x)<+\infty$, then we have the following Talagrand inequality
	\begin{equation}
		W_1\left(\mu,\pi\right)\leq \lambda\sqrt{KL\left(\mu|\pi\right)}
	\end{equation}
\end{lemma}
  
\begin{proof}
	see Villani...
\end{proof}

\begin{lemma}
	\text{E}$\left(\left\|g_n^{\prime}\right\|_{\cH}^2\right)$ and \text{E}$\left(\left(\int|\nabla\log(\pi)(x)|d\mu_n(x)\right)^2+\left(\int|\nabla\log(\sigma_{I^{\prime}})(x)|d\mu_n(x)\right)^2\right)$ are finite.
\end{lemma}

\begin{proof}
	\begin{equation}
		\begin{split}
			\text{E}\left(\left\|g_n^{\prime}\right\|_{\cH}^2\right)& \leq \text{E}\left(\left\|g_{n}^{\prime}-S_{\mu_n}\nabla\log(\frac{\mu_n}{\pi})\right\|_{\cH}+\left\|S_{\mu_n}\nabla\log(\frac{\mu_n}{\pi})\right\|_{\cH}\right)^2\\
			&\leq \text{E}\left(\left\|g_{n}^{\prime}-S_{\mu_n}\nabla\log(\frac{\mu_n}{\pi})\right\|_{\cH}+\left\|S_{\mu_n}\nabla\log(\mu_n)\right\|_{\cH}+\left\|S_{\mu_n}\nabla\log(\pi)\right\|_{\cH}\right)^2\\
			&\leq 3 \text{E}\left(\left\|g_{n}^{\prime}-S_{\mu_n}\nabla\log(\frac{\mu_n}{\pi})\right\|_{\cH}^2\right)+3 \text{E}\left(\left\|S_{\mu_n}\nabla\log(\mu_n)\right\|_{\cH}^2\right)+3 \text{E}\left(\left\|S_{\mu_n}\nabla\log(\pi)\right\|_{\cH}^2\right).
		\end{split}
	\end{equation}

We know the first term on the right hand side is bounded by $3 \Phi_0$, the second term is bounded
 by $3C$ since the derivative of $k$ is bounded by $C$, the third term is bounded by
  $6C\left(|\nabla\log(\pi(0))|\right)^2+6CM\text{E}\left(\int|x|d\mu_n(x)\right)^2$ since the $M$-smoothness of $\log(\pi(x))$.
  
  Similarly, we have 
  \begin{equation}
  	\begin{split}
  		&\quad\text{E}\left(\left(\int|\nabla\log(\pi)(x)|d\mu_n(x)\right)^2+\left(\int|\nabla\log(\sigma_{I^{\prime}})(x)|d\mu_n(x)\right)^2\right) \\
  		&\leq 2\left(|\nabla\log(\pi(0))|\right)^2+2\max_{I^{\prime}}\left(|\nabla\log(\sigma_{I^{\prime}}(0))|\right)^2+4M\text{E}\left(\int|x|d\mu_n(x)\right)^2
  	\end{split}
  \end{equation}
  From Talagrand inequality, we have 
  \begin{equation}
  	\text{E}\left(W_1(\mu_n,\delta_0)-W_1(\delta_0,\pi)\right)^2\leq \text{E}\left(W_1^2(\mu_n,\pi)\right)\leq \lambda\text{E}\left(KL(\mu_n|\pi)\right)\leq \lambda \Phi_0
  \end{equation}
so by Young inequality,
\begin{equation}
	\begin{split}
	\text{E}\left(W_1^2(\mu_n,\delta_0)\right)&\leq \lambda \Phi_0-W_1^2(\delta_0,\pi)+2W_1(\delta_0,\pi)\text{E}\left(W_1(\mu_n,\delta_0)\right)\\
	& \leq \lambda \Phi_0-W_1^2(\delta_0,\pi)+8W_1^2(\delta_0,\pi)+\frac{(\text{E}(W_1(\mu_n,\delta_0)))^2}{4}\\
	& \leq \lambda \Phi_0+7W_1^2(\delta_0,\pi)+\frac{\text{E}(W_1^2(\mu_n,\delta_0))}{4}
	\end{split}
\end{equation}
Finally, we have

\begin{equation}
	\text{E}\left(\int|x|d\mu_n(x)\right)^2=\text{E}(W_1^2(\mu_n,\delta_0))\leq \frac{4}{3}\left(\lambda \Phi_0+7W_1^2(\delta_0,\pi)\right).
\end{equation}
In the end,
\begin{equation}
	\text{E}\left(\left\|g_n^{\prime}\right\|_{\cH}^2\right)\leq 3\Phi_0+3C+6C(|\nabla\log(\pi(0))|)^2+8CM\left(\lambda \Phi_0+7W_1^2(\delta_0,\pi)\right),
\end{equation}
\begin{equation}
	\begin{split}
	&\quad\text{E}\left(\left(\int|\nabla\log(\pi)(x)|d\mu_n(x)\right)^2+\left(\int|\nabla\log(\sigma_{I^{\prime}})(x)|d\mu_n(x)\right)^2\right)\\
	&\leq 2\left(|\nabla\log(\pi(0))|\right)^2+2\max_{I^{\prime}}\left(|\nabla\log(\sigma_{I^{\prime}}(0))|\right)^2+\frac{16}{3}M\left(\lambda \Phi_0+7W_1^2(\delta_0,\pi)\right).
	\end{split}
\end{equation}
 
\end{proof}

From the definition of $\bar{\gamma_n}$, $\tilde{\gamma_n}$ and Lemma 8., we now have
\begin{equation}
	\text{E}\left(\frac{1}{\bar{\gamma_n^2}}\right)\leq C_1,
\end{equation}


\begin{equation}
	\text{E}\left(\frac{1}{\tilde{\gamma_n^2}}\right)\leq C_2,
\end{equation}

So 
\begin{equation}
	\text{E}\left(\frac{1}{\gamma_n^2}\right)\leq \text{E}\left(\frac{1}{\tilde{\gamma_n^2}}\right)+\text{E}\left(\frac{1}{\bar{\gamma_n^2}}\right)\leq C_1+C_2.
\end{equation}
(We can calculate the exact form of $C_1,C_2$, since they are too complicated, we ignore here.)

\begin{lemma}
		We have no less than 0.96 chance such that $\gamma_n\geq \frac{1}{5\sqrt{C_1+C_2}}$, in other words,
		$\text{E}(\gamma_n)\geq \frac{0.96}{5\sqrt{C_1+C_2}}.$
\end{lemma}

\begin{proof}
	if we have more than $\frac{1}{25}$ chance such that $\gamma_n\leq \frac{1}{5\sqrt{C_1+C_2}} $, then $\text{E}(\frac{1}{\gamma_n^2})>\frac{1}{25}25(C_1+C_2)=C_1+C_2$, which contradicts  (48).
\end{proof}


\begin{lemma}
	The expectation of the number $\#$of $=\{k|\gamma_k\geq \frac{1}{5\sqrt{C_1+C_2}},k\leq n\}$ is at least $0.96n$ and the posibility such that this number is bigger than $0.5n$ is at least 0.92.
\end{lemma}

\begin{proof}
	The first argument is easy. If the chance that this number less than $0.5n$ is bigger than 0.08,
	then $\text{E}\left(\#\right)<0.08\times 0.5n+n(1-0.08)=0.96n$ which contradicts the first argument.
\end{proof}

We define $I_{\gamma_n\geq \frac{1}{5\sqrt{C_1+C_2}}}=1$, if $\gamma_n\geq \frac{1}{5\sqrt{C_1+C_2}}$, else $I_{\gamma_n\geq \frac{1}{5\sqrt{C_1+C_2}}}=0$.
So we have 
\begin{equation}
	\begin{split}
	\Phi_0\geq\sum_{i=1}^n\text{E}\left(\frac{\gamma_i}{2}I^2_{Stein}(\mu_i|\pi)\right)&\geq \sum_{i=1}^n\text{E}\left(I_{\gamma_i\geq \frac{1}{5\sqrt{C_1+C_2}}}\frac{\gamma_i}{2}I^2_{Stein}(\mu_i|\pi)\right)\\
	& \geq\frac{1}{10\sqrt{C_1+C_2}}\sum_{i=1}^n\text{E}\left(I_{\gamma_i\geq \frac{1}{5\sqrt{C_1+C_2}}}I^2_{Stein}(\mu_i|\pi)\right)
	\end{split}
\end{equation}
We also know that 
\begin{equation}
	\min_{j\in \{k|\gamma_k\geq \frac{1}{5\sqrt{C_1+C_2}},k\leq n\}}I^2_{Stein}(\mu_j|\pi)\leq I^2_{Stein}(\mu_i|\pi),
\end{equation}
for every $i\in \{k|\gamma_k\geq \frac{1}{5\sqrt{C_1+C_2}},k\leq n\}$, then we know
\begin{equation}
	\begin{split}
		&\quad\text{E}\left(\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\right)\\
		&\leq\text{E}\left(\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\min_{j\in \{k|\gamma_k\geq \frac{1}{5\sqrt{C_1+C_2}},k\leq n\}}I^2_{Stein}(\mu_j|\pi)\right)\\
		&\leq \text{E}\left(I_{\gamma_i\geq \frac{1}{5\sqrt{C_1+C_2}}}I^2_{Stein}(\mu_i|\pi)\right)
	\end{split}
\end{equation}
combine (49) and (51), we derive that
\begin{equation}
	\text{E}\left(\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\right)\leq 10\sqrt{C_1+C_2}\Phi_0.
\end{equation}

so 
\begin{equation}
	\begin{split}
	&\quad \text{E}\left(\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\Big|\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\geq 0.5n\right)\leq \frac{10\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}
	\end{split}
\end{equation}
\begin{lemma}
	We have more than 0.9108 chance such that $\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\leq \frac{1000\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}$.
\end{lemma}
\begin{proof}
	If not , then we have more than 0.01 chance such that $\text{E}\left(\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\Big|\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\geq 0.5n\cap \mathcal{F}\right)> \frac{1000\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}$, then  
	\begin{equation}
		\begin{split}
			\text{E}\left(\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\Big|\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\geq 0.5n\right)&>0.01\times \frac{1000\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}\\
			&= \frac{10\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}
		\end{split}
	\end{equation}
which contradicts (53). So we have more than 0.99 chance such that 
$\text{E}\left(\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\Big|\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\geq 0.5n\cap \mathcal{F}\right)\leq \frac{1000\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}$, so we have more than $0.99\times 0.92=0.9108$ chance such that  $\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\leq \frac{1000\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}$
\end{proof}
























































\end{document}
