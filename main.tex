\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, sort&compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}
\usepackage{xcolor}         % colors


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2020} with \usepackage[nohyperref]{icml2020} above.
\usepackage{hyperref}
\usepackage{times}		
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} 
\DeclareUnicodeCharacter{00A0}{~}
\usepackage{amssymb,amsmath,amscd,amsfonts,amsthm,bbm,mathrsfs,yhmath}

%\usepackage[shortlabels]{enumitem}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{graphics}
\usepackage{mathtools}
\usepackage{cleveref}
\usepackage{comment}

\usepackage{paralist,enumitem}                                       
%\usepackage{tabto}
\usepackage{pdfpages}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{condition}{Condition}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}

\newcommand{\subscript}[2]{$#1 _ #2$}
\newlist{assumplist}{enumerate}{1}
\setlist[assumplist]{label=(\subscript{\textbf{A}}{{\arabic*}})}
\Crefname{assumplisti}{Assumption}{Assumptions}


\newlist{assumplist2}{enumerate}{1}
\setlist[assumplist2]{label=(\subscript{\textbf{B}}{{\arabic*}})}
\setcounter{assumption}{3}
\Crefname{assumplist2i}{Assumption}{Assumptions}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}



\usepackage[textwidth=2cm, textsize=footnotesize]{todonotes}  
%\setlength{\marginparwidth}{1.5cm}               %  this goes with todonoteshttps://fr.overleaf.com/project/5cc847f1dc86b619ccf2295b
\newcommand{\prnote}[1]{\todo[color=cyan!20]{#1}}
\newcommand{\asnote}[1]{\todo[color=green!]{#1}}
\newcommand{\lsnote}[1]{\todo[color=magenta]{#1}}
\newcommand{\adil}[1]{{\color{green!} #1}}
\newcommand{\cF}{{\mathcal F}}
\newcommand{\mcG}{{\mathscr G}}
\newcommand{\cB}{{\mathcal B}}
\newcommand{\cO}{{\mathcal O}}
\newcommand{\cG}{{\mathcal G}}
\newcommand{\cN}{{\mathcal N}}
\newcommand{\cL}{{\mathcal L}}
\newcommand{\cP}{{\mathcal P}}
\newcommand{\X}{{\mathcal X}} 
\newcommand{\Y}{{\mathcal Y}} 
\newcommand{\F}{{\mathcal F}}
\newcommand{\cH}{{\mathcal H}}
\newcommand{\J}{{\mathcal J}}
\newcommand{\G}{{\mathcal G}}
\newcommand{\C}{{\mathcal C}} 
\newcommand{\cS}{{\mathcal S}} 
\newcommand{\M}{{\mathcal M}} 
\newcommand{\E}{{{\mathbb E}}}
\newcommand{\kH}{{{\mathcal H}}} 
\newcommand{\R}{{\mathbb R}} 
\newcommand{\bP}{{\mathbb P}} 
\newcommand{\bE}{{\mathbb E}} 
\newcommand{\sX}{{\mathsf X}} 
\newcommand{\esp}{{\epsilon}} 
\newcommand{\Norm}[1]{\left\|#1\right\|_{H}}
\newcommand{\sqN}[1]{\Norm{#1}^2}

\newcommand{\dom}{{dom}} 
\newcommand{\KL}{\mathop{\mathrm{KL}}\nolimits}
\newcommand{\KSD}{\mathop{\mathrm{KSD}}\nolimits}
\newcommand{\Unif}{\mathop{\mathrm{Unif}}\nolimits}
\newcommand{\HS}{\mathop{\mathrm{HS}}\nolimits}
\newcommand{\op}{\mathop{\mathrm{op}}\nolimits}
\newcommand{\tr}{\mathop{\mathrm{tr}}\nolimits}
\newcommand{\st}{\mathop{\mathrm{Stein}}\nolimits}
\newcommand{\ps}[1]{\langle #1 \rangle}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}



\title{Improved Complexity Analysis of SVGD \\ under Talagrand's Inequality}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
  We study the complexity of SVGD which is an algorithm to sample from $\pi(x) \propto \exp(-F(x))$ where $F$ $L$-smooth and nonconvex. 
\end{abstract}
\section{Introduction}
Sampling from a given target distribution $\pi$ is a fundamental task of many Machine Learning procedures. In Bayesian Machine Learning, the target distribution $\pi$ is known up to a multiplicative factor and often takes the form
\begin{equation}
    \label{eq:target}
    \pi(x) \propto \exp(-F(x)),
\end{equation}
where $F:\X \to \R$ is a function defined on $\X \coloneqq \R^d$.

As sampling algorithms are intended to be applied on large scale problems, it has become more and more important to understand their theoretical properties such as their complexity, as a function of the dimension of the problem $d$ and the accuracy $\varepsilon$ wanted by the user. In this regard, most of the Machine Learning literature has concentrated on understanding the complexity (in terms of $d$ and $\varepsilon$) of (variants of) the Langevin algorithm in various settings, see \textit{e.g.}~\cite{durmus2018analysis,bernton2018langevin,wibisono2018sampling,cheng2017underdamped,salim2020primal,hsieh2018mirrored,dalalyan2017theoretical,durmus2017nonasymptotic,rolland2020double,vempala2019rapid,zou2019sampling,csimvsekli2017fractional,shen2019randomized,bubeck2018sampling,durmus2018efficient}. 

Stein Variational Gradient Descent (SVGD)~\cite{liu2016stein,liu2017stein} is an alternative to the Langevin algorithm that has been applied in several contexts in machine learning, including Reinforcement Learning~\cite{liu2017policy}, sequential decision making~\cite{zhang2018learning,zhang2019scalable}, Generative Adversarial Networks~\cite{tao2019variational}, Variational Auto Encoders~\cite{pu2017vae}, Federated Learning~\cite{kassab2020federated} etc. However, the theoretical understanding of SVGD is limited compared to that of Langevin algorithm~\cite{lu2019scaling,duncan2019geometry,liu2017stein}. In particular, the first complexity result of SVGD appeared recently~\cite[Corollary 6]{korba2020non} under an unverifiable assumption on the trajectory of the algorithm, basically that the first moment of the iterates remain uniformly bounded by a constant $C$ at every iteration. Moreover, the complexity result~\cite[Corollary 6]{korba2020non} does not provide the dependence in the dimension of the problem $d$, precisely because their bound depends on $C$.

Our paper intends to bridge this gap. Our main assumption is that $\pi$ satisfies T1, the mildest of the Talagrand's inequalities, which is satisfied under mild assumptions on the tail of the distribution~\cite[Theorem 22.10]{villani2008optimal} and is implied by Log-Sobolev Inequality (LSI)~\cite[Theorem 22.17]{villani2008optimal}. Under this assumption, 
\begin{itemize}
    \item We provide a convergence rate for SVGD in the so-called population limit without unverifiable assumption on the trajectory of the algorithm
    \item We provide a complexity for SVGD in terms of the dimension $d$ and the desired accuracy $\varepsilon$
    \item We provide a generic weak convergence result for SVGD. 
\end{itemize}
All these results hold without assuming $F$ convex. Our complexity result is summarized in Table~\ref{tab:result}.
\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c| } 
 \hline
 Algorithm & Reference & Assumptions & Criterion & Complexity \\ 
 \hline \hline
 SVGD & This paper, Theorem~\ref{th:svgd} & T1 with constant $\lambda$, $F$ $L$-smooth & $\KSD(\mu|\pi)$ & $\tilde{\cO}\left(\frac{L{d}^{3/2}}{\lambda^2\varepsilon}\right)$\\ 
 \hline
 Langevin & \cite[Theorem 1]{vempala2019rapid} & LSI with constant $\lambda$, $F$ $L$-smooth & $\KL(\mu|\pi)$ & $\tilde{\cO}(\frac{L^2 d}{\lambda^2 \varepsilon})$\\ 
 \hline
\end{tabular}
\label{tab:result}
\caption{Complexity results. Recall that LSI implies T1 with the same constant~\cite[Theorem 22.17]{villani2008optimal}.}
\end{table}
% %HERE
% Both Langevin and SVGD are algorithms to sample from a target distribution $\pi$ whose density is proportional to $\exp(-F)$, where $F$ is a smooth function. They both make use the gradient of the gradient of $F$ and they can both be interpreted as a gradient flow in a space of probability measures~\cite{liu2017,steingeometry,wibisono2018sampling,durmus2018analysis,bernton2018langevin}. However, the theoretical understanding of SVGD is rather limited compared to that of the Langevin algorithm. In particular, the only known convergence rate for SVGD appeared recently~\cite{korba2020non}. However, their complexity analysis relies on an unverfiable assumption on the trajectory of the algorithm (basically that the first moment of the iterates remain uniformly bounded) their complexity result does not provide the dependence on the dimension of the problem. 

% In this work, we assume that the target distribution satisfies a Talagrand's inequality. This assumption allows $F$ to be nonconvex~\cite[]{villani2008optimal}, and is milder than Log Sobolev Inequality that has been used in the analysis of Langevin algorithm~\cite{vempala2019rapid}. Under this assumption, we prove a complexity result that depends on the accuracy $\varepsilon$ and the dimension $d$, similarly to what has been done in the analysis of Langevin algorithm~\cite[Theorem 1]{vempala2019rapid}. We also prove additional convergence results for SVGD. In summary. our contributions are as follows: we study SVGD in the so called population limit, and prove 
% \begin{itemize}
%     \item A convergence rate in terms of the Kernelized Stein Discrepancy (KSD, see below) without making any unverifiable assumption on the trajectory of the algorithm, but by assuming that the target distribution satisfies the mildest of the Talagrand's inequalities, called T1~\cite{villani2008optimal},
%     \item The complexity result $\tilde{\cO}\left(\frac{L{d}^{3/2}}{\lambda^2\varepsilon}\right)$ that gives the sufficient number of iterations to reach $\varepsilon$ accuracy in terms of KSD as a function of the smoothness constant $L$, the dimension $d$ and the T1 constant $\lambda$,
%     \item A generic weak convergence result of SVGD under further assumptions on the target distribution.
% \end{itemize}

% \asnote{Add the log terms to compare to Wibisono Vempala} 

The remainder is organized as follows.


\section{Background}
\subsection{Optimal transport}
%L^2(\mu,\R^d), pushforward, W_1, W_2
\asnote{T1, T2, LSI, strong logconcavity}
\subsection{Reproducing Kernel Hilbert Space}
For any Hilbert space $H$, we denote by $\ps{\cdot,\cdot}_{H}$ the inner product of $H$ and by $\|\cdot\|_{H}$ its norm.
We consider a kernel $k$ associated to a Reproducing Kernel Hilbert Space (RKHS) denoted by $\cH_0$. We denote the so-called feature map $k(x,\cdot)$ by $\Phi(x) \coloneqq k(\cdot,x) \in \cH_{0}$ for every $x \in \X$. The Hilbert space $\cH_0^d$ is denoted by $\cH \coloneqq \cH_0^d$. We make the following assumption.
\begin{assumption}
\label{ass:k_bounded}
$\exists B>0$ s.t. for all $x \in \X$,\\
$\|\Phi(x)\|_{\cH_0}\le B$ and $\|\nabla \Phi(x)\|_{\cH}=\left(\sum_{i=1}^d \|\partial_{i} \Phi(x)\|^2_{\cH_0}\right)^{\frac{1}{2}}\le B$.
%\asnote{+ Mercer kernel and conditions to do the integration by part?}
\end{assumption}
Under Assumption~\ref{ass:k_bounded}, $\cH \subset L^2(\mu)$ for every probability distribution on $\X$, and the inclusion $\iota_{\mu}: \cH \to L^2(\mu)$ is continuous. We denote by $P_{\mu}: L^2(\mu) \to \cH$ its adjoint defined by the relation: for every $f \in L^2(\mu)$, $g \in \cH$, \begin{equation}
    \ps{f, \iota g}_{L^2(\mu)} = \ps{P_{\mu} f, g}_{\cH}.
\end{equation}
Then, $P_\mu$ can be expressed as a convolution with $k$~\cite[Proposition 3]{carmeli2010vector}:
\begin{equation}
    P_{\mu} f(x) = \int k(x,y)f(y)d\mu(y), \quad \text{or},  \quad P_{\mu} f = \int \Phi(y)f(y)d\mu(y)
\end{equation}
where the integral converges in norm.


\subsection{Stein Variational Gradient Descent}

Stein Variational Gradient Descent (SVGD) is an algorithm to sample from $\pi \propto \exp(-F)$. SVGD proceeds by maintaining a set of particles over $\R^d$, whose empirical distribution $\mu_n$ at time $n$ aims to approximate $\pi$ as $n \to \infty$, see~\cite{liu2016stein}. In this paper, we analyze SVGD in the so-called population limit, where the number of particles is infinite. In this limit, the distribution $\mu_n$ follows the dynamics
\begin{equation}
\label{eq:svgdpopulation}
    \mu_{n+1} = \left(I - \gamma h_{\mu_n}\right)\#\mu_n,
\end{equation}
where 
\begin{equation}
    h_{\mu} = P_{\mu} \nabla \log \left(\frac{\mu}{\pi}\right).
\end{equation}
\asnote{Check integration by part here and in the proof of the. descent lemma also. We can show that $\mu_n(x)$ is $C^1$ by induction. In particular we can apply \cite[Lemma 10.4.1]{ambrosio2008gradient}. Moreover, if $\mu_n \in \cP_2(\X)$, then $\mu_{n+1} \in \cP_2(\X)$}
%(x) \coloneqq \int k(x,y)\nabla F(y) - \nabla_y k(x,y) d\mu(y), \quad \text{or},\quad  h_{\mu} \coloneqq \int \nabla F(y) \Phi(y) - \nabla \Phi(y) d\mu(y).
SVGD can be interpreted as a Riemannian gradient descent thanks to two interpretations. First, the Wasserstein space can be interpreted as a Riemannian space~\cite{villani2008optimal} and the exponential map at $\mu$ is $\phi \mapsto (I + \phi)\#\mu$. Second, $\nabla \log \left(\frac{\mu}{\pi}\right)$ can be identified as $\nabla \cF(\mu)$ and $P_{\mu}\nabla \cF(\mu)$ is the ...
Combining these two interpretations, SVGD can be interpreted as a Riemannian gradient descent.
\newcounter{contlist}
\begin{assumplist}
	\setlength\itemsep{0.2em}
		\item \label{ass:k_bounded}
	$\exists B>0$ s.t. for all $x \in \X$,\\
	$\|\Phi(x)\|_{\cH_0}\le B$ and $\|\nabla \Phi(x)\|_{\cH}=(\sum_{i=1}^d \|\partial_{i} \Phi(x)\|^2_{\cH_0})^{\frac{1}{2}}\le B$.\asnote{+ Mercer kernel and conditions to do the integration by part?}
	\item 	\label{ass:V_Lipschitz} The Hessian  $H_{F}$ is well-defined and $\exists M>0$ s.t. for all $i$, $\|H_{F}\|_{\op}\le M$. 
	%Assume  that $\nabla \log \pi$ is $M$-Lipschitz : \\$\|\nabla \log \pi (x) - \nabla \log \pi(y)\|\le M \|x-y\|$ for any $x,y\in \X$.
	\item \label{ass:T1} $\exists \lambda > 0$ s.t. for all $\mu \in \cP_1(\X)$,\asnote{Actually, we only need it for all $\mu \in \cP_2(\X)$? Check this. CHANGE THE DEFINITION OF THE CONSTANT!} $W_1(\mu,\pi) \leq \lambda \sqrt{\KL(\mu|\pi)}$.
	\item 	\label{ass:stationary} $\exists x_\star \in \X$ s.t. $\nabla F(x_\star) = 0$ \textit{i.e.}, $F$ admits a stationary point.
\end{assumplist}
\setcounter{contlist}{\value{enumi}}
% \asnote{Can we extend outside of the population limit? no, locally sobolev}
\Cref{ass:T1} is called Talagrand's inequality T1 and is satisfied if and only if there exist $a \in \X$ and $\beta > 0$ s.t. $\int \exp(\beta \|x-a\|^2)d\pi(x) < \infty,$ see~\cite[Theorem 22.10]{villani2008optimal}. Therefore, \Cref{ass:T1} is essentially an assumption about the tail of $\pi$.

\section{SVGD}
In this section, we analyze SVGD in the infinite number of particles regime. In this regime, SVGD is given by
\begin{equation}
\label{eq:svgd-mean-field}
    \mu_{n+1} = (I - \gamma h_{\mu_n}) \# \mu_n,
\end{equation}
where 
\begin{equation}
    h_{\mu} \coloneqq \int \nabla F(x) \Phi(x) - \nabla \Phi(x) d\mu(x).
\end{equation}
\asnote{Relationship with Wasserstein gradient here. Check: definition of Wass grad. Conditions IPP. $\cP_1$ or $\cP_2$?}
%In this section we study the behavior of the Kullback-Leibler divergence along the Stein Variational Gradient Descent algorithm in discrete time.


% Dependence in $d$:\asnote{Se ramener au first moment de gaussienne + KL between gaussienne e=and target, then apply Lemma 1 of Vempala Wibisono. Dependence of lambda in d? KLS? If strongly log concave, $\lambda = $ racine du parametre de strong convexity}

%In this section we assume \Cref{ass:k_bounded} holds, i.e. the norms of the kernel $k(x,x)$ and $  \nabla_1.\nabla_2 k(x,x)$ are bounded by some positive constant $B^2$.  We will also rely on \Cref{ass:bounded_I_Stein}, that states that the Stein Fisher information remains bounded at all iterations by some $C>0$.

%\subsection{A descent lemma}

%The following lemma states that the boundedness of the kernel, its gradient and the Hessian of $\pi$, as well as a the moments along the trajectory, are sufficient to satisfy the boundedness of the Stein Fisher information for all $n\ge0$. 

%  Under \Cref{ass:V_Lipschitz,ass:k_bounded}, a sufficient condition for~\Cref{ass:bounded_I_Stein} is $\sup_n \int \Vert x \Vert \mu_n(x)dx < \infty$. Bounded moment assumptions such as these are commonly used in stochastic optimization, for instance in some analysis of the stochastic gradient descent~\citep{moulines2011non}. Given our assumptions, we quantify the decreasing of the KL along the SVGD algorithm, also called a descent lemma in optimization.
 

\begin{theorem}
\label{th:svgd}
  Let Assumptions \ref{ass:k_bounded}, \ref{ass:V_Lipschitz} and \ref{ass:T1} hold true. Let $\alpha > 1$. 
  
  If 
  \begin{equation}
  \label{eq:condition-step}
      \gamma \leq (\alpha-1)\left(\alpha B^2 \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x) + M \lambda \sqrt{\KL(\mu_0|\pi)} \right)\right)^{-1},
  \end{equation}
  OR if \Cref{ass:stationary} holds and
  \begin{equation}
  \label{eq:condition-step-2}
      \gamma \leq (\alpha-1)\left(\alpha B^2 \left(1 + 2M\lambda \sqrt{\cF(\mu_0)} + M \int \|x-x_\star\|d\mu_0(x))\right)\right)^{-1},
  \end{equation}
  then, 
  \begin{equation}
    \label{eq:TL-svgd-cst}
        \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \gamma\left(1 - \frac{\gamma B (\alpha^2 + M)}{2}\right)I_{\st}(\mu_n|\pi).
    \end{equation}
\end{theorem}
%  We provide here a sketch of the proof and the main ideas, the reader may refer to the \Cref{sec:proof_descent}  %\Cref{sec:proof_descent} 
%  for the complete proof.

\section{General convergence}
\begin{corollary}[Convergence]
Let Assumptions \ref{ass:k_bounded}, \ref{ass:V_Lipschitz} and \ref{ass:T1} hold true. Assume moreover that $I_{\st}(\mu_n|\pi) \rightarrow_{n \to +\infty} 0 \Rightarrow \mu_n \rightarrow_{n \to +\infty} \pi$ weakly. Let $\alpha > 1$. 
  If 
  \begin{equation*}
      \gamma \leq (\alpha-1)\left(\alpha B^2 \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x) + M \lambda \sqrt{\KL(\mu_0|\pi)} \right)\right)^{-1}\quad \text{and} \quad \gamma < \frac{2}{B (\alpha^2 + M)},
  \end{equation*}
  then $\mu_n \rightarrow_{n \to +\infty} \pi$ weakly.
\end{corollary}
\begin{proof}
    Using Theorem~\ref{th:svgd} and iterating, 
    \begin{equation*}
        \cF(\mu_{n}) \leq \cF(\mu_{0}) - \gamma\left(1 - \frac{\gamma B (\alpha^2 + M)}{2}\right) \sum_{k=0}^{n-1} I_{\st}(\mu_k|\pi),
    \end{equation*}
    therefore, for every $n \geq 0$,
    \begin{equation*}
    \gamma\left(1 - \frac{\gamma B (\alpha^2 + M)}{2}\right) \sum_{k=0}^{n-1} I_{\st}(\mu_k|\pi) \leq \cF(\mu_0).
    \end{equation*}
    Therefore, $\sum_{n=0}^{+\infty} I_{\st}(\mu_n|\pi) < \infty$. Therefore $I_{\st}(\mu_n|\pi) \rightarrow_{n \to +\infty} 0$ and $\mu_n \rightarrow_{n \to +\infty} \pi$ weakly.
\end{proof}
Conditions under which $I_{\st}(\mu_n|\pi) \rightarrow_{n \to +\infty} 0 \Rightarrow \mu_n \rightarrow_{n \to +\infty} \pi$ can be found in~\cite[Theorem 8]{gorham2017measuring}. In particular, a sufficient condition is the combines the two following properties:
\begin{itemize}
    \item $\pi$ being distant dissipative. For instance, $\pi$ is a finite Gaussian mixture with common covariance or $F$ is strongly convex outside a compact set (note that in this case, \Cref{ass:T1} is satisfied using~\cite[Theorem 22.10]{villani2008optimal})
    \item $k$ is an inverse multiquadratic kernel, \textit{i.e.}, $k(x,y) = (c^2 + \|x-y\|^2)^\beta$ for some $c > 0$ and $\beta \in (0,1)$ (note that \Cref{ass:k_bounded} is satisfied).\asnote{Check these!}
\end{itemize}  
%(note that such kernel satisfies \Cref{ass:k_bounded})\asnote{Check this}.

\section{Complexity}
\begin{corollary}[Convergence rate]
Let Assumptions \ref{ass:k_bounded}, \ref{ass:V_Lipschitz} and \ref{ass:T1} hold true. Let $\alpha > 1$. 
  If 
  \begin{equation*}
      \gamma \leq \min\left((\alpha-1)\left(\alpha B^2 \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x) + M \lambda \sqrt{\KL(\mu_0|\pi)} \right)\right)^{-1},\frac{1}{B (\alpha^2 + M)}\right),
  \end{equation*}
  OR if \Cref{ass:stationary} holds and
  \begin{equation*}
      \gamma \leq \min\left((\alpha-1)\left(\alpha B^2 \left(1 + 2M\lambda \sqrt{\cF(\mu_0)} + M \int \|x-x_\star\|d\mu_0(x))\right)\right)^{-1},\frac{1}{B (\alpha^2 + M)}\right),
  \end{equation*}
  then,
  \begin{equation}
      \frac{1}{n} \sum_{k=0}^{n-1} I_{\st}(\mu_k|\pi) \leq \frac{2\cF(\mu_0)}{n \gamma}.
  \end{equation}
\end{corollary}
\begin{proof}
    Using Theorem~\ref{th:svgd}, 
    \begin{equation*}
        \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \frac{\gamma}{2} I_{\st}(\mu_n|\pi),
    \end{equation*}
    and by iterating,
    \begin{equation*}
        0 \leq \cF(\mu_{n}) \leq \cF(\mu_{0}) - \frac{\gamma}{2} \sum_{k=0}^{n-1} I_{\st}(\mu_k|\pi).
    \end{equation*}
    We obtain the result by rearranging the terms.
\end{proof}
\begin{corollary}[Complexity]
\label{cor:comp}
Let Assumptions \ref{ass:k_bounded}, \ref{ass:V_Lipschitz}, \ref{ass:T1} and \ref{ass:stationary} hold true. Let $\alpha > 1$. If
\begin{equation*}
      \gamma \leq \min\left((\alpha-1)\left(\alpha B^2 \left(1 + 2M\lambda \sqrt{F(x_\star) + \frac{d}{2}\log \left(\frac{M}{2\pi}\right)} + \sqrt{M d}\right)\right)^{-1},\frac{1}{B (\alpha^2 + M)}\right),
  \end{equation*}
  and if $\mu_0 = \cN(x_\star,\frac{1}{L} I)$,
  then $n = \tilde{\cO}\left(\frac{M\lambda{d}^{3/2}}{\varepsilon}\right)$ iterations of SVGD are sufficient to obtain $\mu$ such that $I_{\st}(\mu|\pi) \leq \varepsilon.$
\end{corollary}


\subsection{Related works}


% We do not have a rate in KL for SVGD but we can have a rate in KSD for Langevin:
% \begin{proposition}[Comparison to Langevin]
% Assume LSI. Then, Langevin $\tilde{\cO}(\frac{L^2 d}{\lambda^2 \varepsilon^2})$
% \end{proposition}
% \begin{proof}
%     Since LSI holds, then T1 holds. Then we can combine Lemma~\ref{lem:T1} with~\cite[Theorem 1]{vempala2019rapid}.
% \end{proof}


To our knowledge, the only existing complexity result for SVGD is~\cite{korba2020non}. They showed that, in the population limit, $\cO(\frac{L}{\varepsilon})$ iterations of SVGD algorithm are sufficient to achieve $\varepsilon$ accuracy in terms of Kernelized Stein Discrepancy (KSD). However, this complexity result does not express the dependence in the dimension $d$. Moreover, this complexity result is established under the assumption that the first moment of the iterates is uniformly bounded, and this uniform bound appears in the hidden constants.

The ML literature on the complexity of sampling from a non logconcave target distribution has many focused on Langevin algorithm. In particular, \cite{vempala2019rapid} showed that $\tilde{\cO}(\frac{L^2 d}{\lambda^2 \varepsilon})$ iterations of Langevin algorithm are sufficient to achieve $\varepsilon$ accuracy in terms of KL divergence if the target distribution satisfies Log Sobolev Inequality with constant $\lambda$. 

Recall that $F$ $\lambda$-strongly convex $\Longrightarrow$ Log Sobolev Inequality with constant $\lambda$ $\Longrightarrow$ Talagrand's inequality T2 with constant $\frac{2}{\lambda^2}$ $\Longrightarrow$ Talagrand's inequality T1 with constant $\frac{2}{\lambda^2}$.


\section{SVSGD}
\section{VR-SVSGD}
In this section, we analyze VR-SVSGD in the infinite number of particles regime. In this regime, VR-SVSGD is given by
\begin{equation}
\label{eq:svgd-mean-field}
    \mu_{n+1} = (I - \gamma g_n) \# \mu_n,
\end{equation}
where 
\begin{equation}
    g_n = v_n-\int \nabla \Phi(x) d\mu_n(x),
\end{equation}
and,
\begin{equation}
    v_{n+1} = \left\{
    \begin{array}{ll}
        P_{\mu_{n+1}} \nabla F & \mbox{with probability } p \\
        v_n + P_{\mu_{n+1}} \nabla f_{I_{n+1}} - P_{\mu_{n}} \nabla f_{I_{n+1}} & \mbox{with probability } 1-p, 
    \end{array}
\right.
\end{equation}
where $(I_n)$ is a sequence of i.i.d. random variables with uniform distribution over $\{1,\ldots,m\}$.

% \begin{theorem}
% \label{th:vrsvsgd}
%   Let Assumptions \ref{ass:k_bounded}, \ref{ass:V_Lipschitz} and \ref{ass:T1} hold true. Let $\alpha > 1$ and choose $\gamma > 0$ such that 
%   \begin{equation}
%   \label{eq:condition-step}
%       \gamma \leq \min\left((\alpha-1)\left(\alpha B^2 \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x) + M \lambda \sqrt{\KL(\mu_0|\pi)} \right)\right)^{-1}, ? \right)
%   \end{equation}
%   Then, 
%   \begin{equation}
%     \label{eq:TL-svgd-cst}
%         \cL_{n+1} \leq \cL_{n} - \gamma\left(1 - \frac{\gamma B (\alpha^2 + M)}{2}\right)I_{\st}(\mu_n|\pi).
%     \end{equation}
% \end{theorem}

\asnote{unbiasedness, complexity, computation of full gradient at the beginning}
\bibliographystyle{plain}
\bibliography{math}


\appendix


\section{Proofs related to SVGD}

In this section we prove our main results (Theorems~\ref{th:svgd} and~\ref{th:vrsgsgd}).

\subsection{Notations}

%%%%NOTATIONS TO DEFINE IN APPENDIX OR MAIN PAPER:
%\cF
%Definition in appendix vs deifnition in main paper
%Define: I, \X, \cH, \cH_0 (and their norms) \cF, P_{\mu_n}\nabla \log\left(\frac{\mu_n}{\pi}\right) = h_{\mu_n} tjrs bien defini, Wass grad under the metric of H cf my personal view of SVGD.
%Jacobian, H_0, HS, op, several inner products, Wasserstein gradients,/Hessian
%Feature map \Phi(x)
%W_1, W_2, Dirac

\subsection{Fundamental inequality}

We start by stating a fundamental inequality satisfied by $\cF$ for any update of the form
\begin{equation}
    \mu_{n+1} = \left(I - \gamma g\right) \# \mu_n,
\end{equation}
where $g \in \cH$.
\begin{proposition}
\label{prop:TL}
Let Assumptions~\ref{ass:k_bounded} and~\ref{ass:V_Lipschitz} hold true. Let $\alpha > 1$ and choose $\gamma > 0$ such that $\gamma \|g\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$. Then,
    \begin{equation}
    \label{eq:TL}
        \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \gamma\ps{h_{\mu_n},g}_{\cH} +  \frac{\gamma^2 K}{2}\|g\|_{\cH}^2,
    \end{equation}
    where $K = (\alpha^2 + M)B$.
\end{proposition}
Inequality~\eqref{eq:TL} plays the role of a Taylor inequality, where $h_{\mu_n}$ is the Wasserstein gradient of $\cF$ at $\mu_n$ under the metric induced by $\cH$. Proposition~\ref{prop:TL} generalizes \cite[Proposition 5]{korba2020non}. Indeed, \cite[Proposition 5]{korba2020non} can be obtained from our Proposition~\ref{prop:TL} by taking $g = h_{\mu_n}$, by assuming the uniform upper bound $\|h_{\mu_n}\|_{\cH}^2 \leq C$ (see \cite[Assumption A3]{korba2020non}) and by restricting the step size to $\gamma C^{\frac{1}{2}} \leq \frac{\alpha-1}{\alpha B}$. Note that assuming $\gamma C^{\frac{1}{2}} \leq \frac{\alpha-1}{\alpha B}$ is more restrictive than assuming $\gamma \|h_{\mu_n}\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$. 

Yet, the proof of our Proposition~\ref{prop:TL} is similar to the proof of \cite[Proposition 5]{korba2020non}, by replacing their $h_{\mu_n}$ by our $g$ and their uniform upper bound $C$ by our $\|g\|_{\cH}^2$. Therefore, we only sketch the main arguments, and further details can be found in \cite[Section 11.2]{korba2020non}
\begin{proof}
    Let $\phi_t = I - t g$ for $t \in [0,\gamma]$ and $\rho_t = (\phi_t) \# \mu_n$. Note that $\rho_0 = \mu_n$ and $\rho_{\gamma} = \mu_{n+1}$. 
    First, for every $x \in \X$,
    \begin{equation}
    \label{eq:gbound}
	\|g(x)\|^2=\sum_{i=1}^d \ps{k(x,.),g_i}^2_{\cH_0} \le \|k(x,.)\|_{\cH_0}^2 \|g\|_{\cH}^2\le B^2 \|g\|_{\cH}^2,
	\end{equation}
	and,
	\begin{align}
	\label{eq:Jgbound}
	\|Jg(x)\|_{\HS}^2&=\sum_{i,j=1}^d \left|\frac{\partial g_i(x)}{\partial x_j} \right|^2=\sum_{i,j=1}^d \ps{\partial_{x_j}k(x,.), g_i}_{\cH_0}^2\le \sum_{i,j=1}^d \| \partial_{x_j}k(x,.)\|^2_{\cH_0} \|g_i\|_{\cH_0}^2 \nonumber\\
	&=\| \nabla k(x,.)\|^2_{\cH}\|g\|^2_{\cH}\le B^2 \|g\|^2_{\cH}.
	\end{align}
    Hence, 
    \begin{equation}
    \label{eq:invers-glob}
        \|t Jg(x)\|_{\op} \leq \|t Jg(x)\|_{\HS} \leq \gamma B \|g\|_{\cH} \leq \frac{\alpha-1}{\alpha} < 1,
    \end{equation} 
    using our assumption on the step size $\gamma$. Inequality~\eqref{eq:invers-glob} proves that $\phi_t$ is a diffeomorphism for every $t \in [0,\gamma]$. Moreover, 
    \begin{equation}
    \label{eq:alpha}
		\|(J\phi_t(x))^{-1}\|_{\op} \leq \sum_{k=0}^\infty \|t Jg(x)\|_{\op}^k \leq \sum_{k=0}^\infty \left(\frac{\alpha-1}{\alpha}\right)^k = \alpha.
    \end{equation}
    Using~\cite[Theorem 5.34]{villani2003topics}, the velocity field ruling the time evolution of $\rho_t$ is $w_t \in L^2(\rho_t)$ defined by $w_t(x) = -g(\phi_t^{-1}(x))$. %Note that $w_0 = -g\in \cH$.    
    
    Denote $\varphi(t) = \cF(\rho_t)$. Using a Taylor expansion,
        \begin{equation}
        \label{eq:Taylor}
            \varphi(\gamma) = \varphi(0) + \gamma \varphi'(0) + \int_{0}^{\gamma} (\gamma - t)\varphi''(t)dt.
        \end{equation}
        We now identify each term. First, 
        \begin{equation*}
    \varphi(0) = \cF(\mu_n)\; \text{ and } \;\varphi(\gamma) = \cF(\mu_{n+1}).
        \end{equation*} 
        Then, using the chain rule~\cite[Section 8.2]{villani2003topics}, 
        \begin{equation*}
            \varphi'(t)=\ps{\nabla_{W_2} \cF(\rho_t),w_t}_{L^2(\rho_t)}\; \text{ and } \;\varphi''(t) = \ps{w_t,H_{W_2}{\cF}(\rho_t)w_t}_{L^2(\rho_t)}.
        \end{equation*}
        Therefore, using $g \in \cH$,
        \begin{equation*}
            \varphi'(0) = -\ps{\nabla_{W_2} \cF(\mu_n),g}_{L^2(\mu_n)} = -\ps{h_{\mu_n},g}_{\cH}.
        \end{equation*}
        Moreover, $\varphi''(t) = \psi_1(t) + \psi_2(t)$, where
        \begin{equation*}
        \psi_1(t) = \E_{x \sim \rho_t} \left[ \ps{w_t(x), H_V(x) w_t(x)}\right] \; \text{ and } \; \psi_2(t) = \E_{x \sim \rho_t} \left[ \|J w_t(x)\|_{\HS}^2 \right].
        \end{equation*}
        Recall that $w_t = -g \circ (\phi_t)^{-1}$.
    The first term $\psi_1(t)$ is bounded using the transfer lemma, \Cref{ass:V_Lipschitz} and Inequality~\eqref{eq:gbound}:
    \begin{equation*}
        \psi_1(t) = \E_{x \sim \mu_n} \left[ \ps{g(x), H_V(\phi_t(x)) g(x)}\right] \leq M \|g\|_{L^2(\mu_n)}^2 \leq M B^2 \|g\|_{\cH}^2.
    \end{equation*} 
    For the second term $\psi_2(t)$, using the chain rule, $-J w_t \circ \phi_t = Jg (J \phi_t)^{-1}$. Therefore, 
    \begin{equation*}
        \|J w_t\circ \phi_t(x)\|_{\HS}^2 \leq \|Jg(x)\|_{\HS}^2 \|(J \phi_t)^{-1}(x)\|_{\op}^2 \leq \alpha^2 B^2 \|g\|_{\cH}^2,
    \end{equation*}
    using \eqref{eq:Jgbound} and \eqref{eq:alpha}.
    %The second term , the first term is easily bounded by
    % $\ps{v,H_V v} \leq M \|v\|^2$. Then since $\rho_t = \phi_{t\#} \mu_n$ and $v_t = -g(\phi_t^{-1})$, by the transfer lemma $\mathbb{E}_{x \sim \rho_t}[\| v_t(x)\|^2]=\mathbb{E}_{x\sim \mu_n}[\|g(x)\|^2]\le B^2I_{Stein}(\mu_n|\pi)$, hence 
    %$
    %\E_{x\sim \rho_t}[ \ps{v_t(x), H_V(x) v_t(x)}]\le M B^2 I_{Stein}(\mu_n|\pi)$.
    %  The second term is the most challenging to bound.  
    %  By the chain rule 
    %  %for any $x$ we have $-J v_t(x)=Jg(\phi_t^{-1}(x))(J\phi_t)^{-1}(\phi_t^{-1}(x))$; hence by 
    %  and the transfer lemma, we first have
    %   $\E_{x \sim \rho_t} \left[\|J v_t(x)\|_{HS}^2\right]= \E_{x \sim \mu_n} \left[\|Jg(x) (J\phi_t)^{-1}(x)\|_{HS}^2\right]$. Then,  for any $x$, $ \|Jg(x) (J\phi_t)^{-1}(x)\|_{HS}^2 \le B^2I_{Stein}(\mu_n|\pi) \| (J\phi_t)^{-1}(x)\|_{op}^2$. On the other hand, $J\phi_t=I-tJg$, and if $t<\frac{1}{B\sqrt{C}}$ then $t\|Jg(x)\|<1$ and it can be shown that $\|(I - t Jg(x))^{-1}\| \le \alpha$ for $\gamma$ chosen as in \Cref{prop:descent}. Therefore $\|Jg(x) (J\phi_t)^{-1}(x)\|_{HS}^2\le \alpha^2 B^2 I_{Stein}(\mu_n|\pi)$ and $\varphi''(t)\le (\alpha^2+M)B^2I_{Stein}(\mu_n|\pi)$. 
    Combining each of the quantity in the Taylor expansion~\eqref{eq:Taylor} gives the desired result.
        %Sufficiently, we shall bound the term $\|Jg)(x) J(\phi_t)^{-1}(x)\|_{HS}^2$. 

\end{proof}
\subsection{Proof of Theorem~\ref{th:svgd}}
As mentioned in the last section, Proposition~\ref{prop:TL} can be applied to SVGD, \textit{i.e.}, the update 
\begin{equation*}
    \mu_{n+1} = (I - \gamma h_{\mu_n}) \# \mu_n,
\end{equation*}
by setting $g = h_{\mu_n} \in \cH$. In this case, we obtain the following.
\begin{lemma}
\label{lem:svgd}
Let Assumptions~\ref{ass:k_bounded} and~\ref{ass:V_Lipschitz} hold true. Let $\alpha > 1$ and choose $\gamma > 0$ such that $\gamma \|h_{\mu_n}\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$. Then,
    \begin{equation}
    \label{eq:TL-svgd}
        \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \gamma\left(1 - \frac{\gamma K}{2}\right)\|h_{\mu_n}\|_{\cH}^2,
    \end{equation}
    where $K = (\alpha^2 + M)B$.
\end{lemma}

\begin{lemma}
\label{lem:T1}
     Let Assumptions \ref{ass:k_bounded}, \ref{ass:V_Lipschitz} and \ref{ass:T1} hold true. Then, for every $\mu \in \cP_2(\X)$,\asnote{$\cP_2$ ?}
\begin{equation*}
    \|h_{\mu}\|_{\cH} \leq B \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x)\right) + BM \lambda \sqrt{\cF(\mu)}.
\end{equation*}
Moreover, if \Cref{ass:stationary} holds, then
\begin{equation*}
\|h_\mu\|_{\cH} \leq B \left(1 + M\lambda \sqrt{\cF(\mu_0)} + M\lambda \sqrt{\cF(\mu)} + M \int \|x-x_\star\|d\mu_0(x)\right).
\end{equation*}
\end{lemma}
\begin{proof}
Using \Cref{ass:k_bounded}
    \begin{align*}
        \|h_{\mu}\|_{\cH} &= \left\|\E_{x \sim \mu} \left(\nabla F(x)\Phi(x) - \nabla \Phi(x)\right) \right\|_{\cH} \\
        &\leq \E_{x \sim \mu} \left\| \nabla F(x)\Phi(x) - \nabla \Phi(x) \right\|_{\cH}\\
        &\leq \E_{x \sim \mu} \left\| \nabla F(x)\Phi(x)\right\|_{\cH} + \E_{x \sim \mu} \left\|\nabla \Phi(x) \right\|_{\cH}\\
        &= \E_{x \sim \mu} \left\|\nabla F(x)\right\|\left\|\Phi(x)\right\|_{\cH} + \E_{x \sim \mu} \left\|\nabla \Phi(x) \right\|_{\cH}\\
        &\leq B \left( \E_{x \sim \mu} \left\|\nabla F(x)\right\| + 1 \right).
    \end{align*}
Using \Cref{ass:V_Lipschitz}, $\|\nabla F(x)\| \leq \|\nabla F(0)\| + M\|x\|$. Therefore, using the triangle inequality for the metric $W_1$,
    \begin{align*}
        \|h_{\mu}\|_{\cH} &\leq B \left(1 + \|\nabla F(0)\| + M\int \|x\|d\mu(x) \right)\\
        &= B \left(1 + \|\nabla F(0)\| + M W_1(\mu,\delta_0) \right)\\
        &\leq B \left(1 + \|\nabla F(0)\| + M W_1(\pi,\delta_0) \right) + BM W_1(\mu,\pi).
    \end{align*}
We obtain the first inequality using \Cref{ass:T1}: $W_1(\mu,\pi) \leq \lambda \sqrt{\cF(\mu)}$.

To prove the second inequality, recall that
\begin{equation*}
\|h_\mu\|_{\cH} \leq B \left( \E_{x \sim \mu} \left\|\nabla F(x)\right\| + 1 \right).
\end{equation*}
Using \Cref{ass:V_Lipschitz} and \ref{ass:stationary}, $\|\nabla F(x)\| = \|\nabla F(x) - \nabla F(x_\star)\| \leq M\|x- x_\star\|$. Therefore, using the triangle inequality for the metric $W_1$,
\begin{align*}
    \int \|x-x_\star\|d\mu(x) &= W_1(\mu,\delta_{x_\star}) \\
    &\leq W_1(\mu,\pi) + W_1(\pi,\mu_0) + W_1(\mu_0,\delta_{x_\star})\\
    &\leq \lambda \sqrt{\cF(\mu_0)} + \lambda \sqrt{\cF(\mu)} + W_1(\mu_0,\delta_{x_\star}).
\end{align*}
Therefore, 
\begin{align}
\label{eq:cond2}
\|h_\mu\|_{\cH} &\leq B \left(1 + M\int \|x-x_\star\|d\mu(x) \right)\nonumber\\
&\leq B \left(1 + M\lambda \sqrt{\cF(\mu_0)} + M\lambda \sqrt{\cF(\mu)} + M W_1(\mu_0,\delta_{x_\star})\right).
\end{align}
\end{proof}


    We now prove by induction the first implication of Theorem~\ref{th:svgd}: \eqref{eq:condition-step} $\Rightarrow$ \eqref{eq:TL-svgd-cst}.
    First, if $\gamma > 0$ satisfies \eqref{eq:condition-step}, then, using Lemma~\ref{lem:T1}, $\gamma \|h_{\mu_0}\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$. Therefore, using Lemma~\ref{lem:svgd}, 
    \begin{equation*}
        \cF(\mu_{1}) \leq \cF(\mu_{0}) - \gamma\left(1 - \frac{\gamma K}{2}\right)\|h_{\mu_0}\|_{\cH}^2,
    \end{equation*}
\textit{i.e.}, Inequality~\eqref{eq:TL-svgd-cst} holds with $n = 0$. 

Now, assume that the condition~\eqref{eq:condition-step} implies Inequality~\eqref{eq:TL-svgd-cst} for every $n \in \{0,\ldots,N-1\}$. Then, $\cF(\mu_N) \leq \cF(\mu_0)$ and
\begin{align*}
    &B \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x)\right) + BM \lambda \sqrt{\cF(\mu_N)}\\ \leq &B \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x)\right) + BM \lambda \sqrt{\cF(\mu_0)}. 
\end{align*}
Therefore, if $\gamma > 0$ satisfies \eqref{eq:condition-step}, then $\gamma \|h_{\mu_N}\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$. Indeed, using Lemma~\ref{lem:T1}
\begin{align*}
\gamma \|h_{\mu_N}\|_{\cH}
&\leq  \gamma \left(B \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x)\right) + BM \lambda \sqrt{\cF(\mu_N)} \right) \\
       &\leq \gamma \left(B \left(1 + \|\nabla F(0)\| + M \int \|x\|d\pi(x)\right) + BM \lambda \sqrt{\cF(\mu_0)} \right) \\
       &\leq \frac{\alpha-1}{\alpha B}.
  \end{align*}
  Therefore, using Lemma~\ref{lem:svgd}, the condition~\eqref{eq:condition-step} implies Inequality~\eqref{eq:TL-svgd-cst} step $N$:
  \begin{equation*}
        \cF(\mu_{N+1}) \leq \cF(\mu_{N}) - \gamma\left(1 - \frac{\gamma K}{2}\right)\|h_{\mu_N}\|_{\cH}^2.
    \end{equation*}
    Finally, it remains to recall that $\|h_{\mu_N}\|_{\cH}^2 = I_{\st}(\mu_N|\pi)$.
    
    The proof of the second implication of Theorem~\ref{th:svgd} \eqref{eq:condition-step-2} $\Rightarrow$ \eqref{eq:TL-svgd-cst} is similar.
    
    \subsection{Proof of Corollary~\ref{cor:comp}}
%     The proof of Corollary~\ref{cor:comp} relies on the following 2nd version of Theorem~\ref{th:svgd}.
%     \begin{theorem}
% \label{th:svgd2}
%   Let Assumptions \ref{ass:k_bounded}, \ref{ass:V_Lipschitz}, \ref{ass:T1} and \ref{ass:stationary} hold true. Let $\alpha > 1$ and choose $\gamma > 0$ such that 
%   \begin{equation}
%   \label{eq:condition-step-2}
%       \gamma \leq (\alpha-1)\left(\alpha B^2 \left(1 + 2M\lambda \sqrt{\cF(\mu_0)} + M W_1(\mu_0,\delta_{x_\star})\right)\right)^{-1}.
%   \end{equation}
%   Then, 
%   \begin{equation}
%     \label{eq:TL-svgd-cst-2}
%         \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \gamma\left(1 - \frac{\gamma B (\alpha^2 + M)}{2}\right)I_{\st}(\mu_n|\pi).
%     \end{equation}
% \end{theorem}
%\begin{proof}
     



Using Corollary~\ref{cor:comp}, if
  \begin{equation*}
      \gamma \leq \min\left((\alpha-1)\left(\alpha B^2 \left(1 + 2M\lambda \sqrt{\cF(\mu_0)} + M \int \|x-x_\star\|d\mu_0(x))\right)\right)^{-1},\frac{1}{B (\alpha^2 + M)}\right),
  \end{equation*}
  then, denoting $p = \argmin_{k \in \{0,\ldots,n-1\}} I_{\st}(\mu_k|\pi)$,
  \begin{equation}
      I_{\st}(\mu_p|\pi) = \frac{1}{n} \sum_{k=0}^{n-1} I_{\st}(\mu_k|\pi) \leq \frac{2\cF(\mu_0)}{n \gamma}.
  \end{equation}
% then,
%     \begin{equation}
%     \label{eq:TLproof}
%         \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \gamma\left(1 - \frac{\gamma K}{2}\right)\|h_{\mu_n}\|_{\cH}^2.
%     \end{equation}
% We conclude by induction as in the proof of Theorem~\ref{th:svgd}: Inequality \eqref{eq:TL-svgd-cst-2} holds for $n=0$, and if it holds up to $n-1$, then $\cF(\mu_n) \leq \cF(\mu_0)$, therefore 
% \begin{align*}
%     &(\alpha-1)\left(\alpha B^2 \left(1 + M\lambda \sqrt{\cF(\mu_0)} + M\lambda \sqrt{\cF(\mu_n)} + M W_1(\mu_0,\delta_{x_\star})\right)\right)^{-1}\\
%     \leq&(\alpha-1)\left(\alpha B^2 \left(1 + 2M\lambda \sqrt{\cF(\mu_0)} + M W_1(\mu_0,\delta_{x_\star})\right)\right)^{-1},
% \end{align*}
% and any $\gamma >0$ satisfying~\eqref{eq:condition-step-2} satisfies~\eqref{eq:condition-step-n}, therefore~\eqref{eq:TLproof} holds.
% \end{proof}
Using~\cite[Lemma 1]{vempala2019rapid}, $\cF(\mu_0) \leq F(x_\star) + \frac{d}{2}\log \left(\frac{M}{2\pi}\right)$. Besides,
\begin{equation}
    \int \|x-x_\star\|d\mu_0(x) = \E_{X \sim \mu_0} \|X - x_\star\| = \frac{1}{\sqrt{M}} \E_{X \sim \mu_0} \|\sqrt{M}(X - x_\star)\|,
\end{equation}
and using the transfer lemma and Cauchy-Schwartz inequality,
\begin{equation}
    \int \|x-x_\star\|d\mu_0(x) = \frac{1}{\sqrt{M}} \E_{Y \sim \cN(0_{\X},I_{\X})} \|Y\| \leq \frac{1}{\sqrt{M}} \left(\E_{Y \sim \cN(0_{\X},I_{\X})} \|Y\|^2\right)^{1/2} = \sqrt{\frac{d}{M}}.
\end{equation}
Therefore,
\begin{equation}
    (\alpha-1)\left(\alpha B^2 \left(1 + 2M\lambda \sqrt{\cF(\mu_0)} + M \int \|x-x_\star\|d\mu_0(x))\right)\right)^{-1} = \tilde{\cO}\left(\frac{1}{M\lambda\sqrt{d} + \sqrt{Md}}\right),
\end{equation}
and 
\begin{equation}
    \gamma^{-1} = \tilde{\cO}\left(M\lambda\sqrt{d} + \sqrt{Md} + M\right) = \tilde{\cO}\left(M\lambda\sqrt{d}\right).
\end{equation}
Since $\cF(\mu_0) = \tilde{\cO}(d)$,
\begin{equation}
    \frac{\cF(\mu_0)}{\gamma} = \tilde{\cO}\left(M\lambda{d}^{3/2}\right).
\end{equation}
Let $\varepsilon > 0$. To output $\mu_p$ such that $I_{\st}(\mu_p|\pi)$, it suffices to ensure that $\frac{2\cF(\mu_0)}{n\gamma} \leq \varepsilon$, therefore, $\frac{2\cF(\mu_0)}{\gamma \varepsilon} = \tilde{\cO}\left(\frac{M\lambda{d}^{3/2}}{\varepsilon}\right)$ iterations are sufficient.

\section{Proofs related to SVSGD}
We now study VR-SVSGD, \textit{i.e.}, the update 
\begin{equation*}
    \mu_{n+1} = (I - \gamma g_n) \# \mu_n,
\end{equation*}
where $g_n = P_{\mu_n} \left(v_n + \nabla \log(\mu_n)\right)$ and $v_n$ is a stochastic gradient, \textit{i.e.}, $\E_n v_n(\cdot) = \nabla F(\cdot)$. In particular, we have $\E_n g_n = h_{\mu_n}$. For simplicity, we assume that the stochastic gradient has a bounded variance, \textit{i.e.}, $\E \|P_{\mu_n} v_n - P_{\mu_n} \nabla F\|_{\cH} \leq \sigma$. By applying Proposition~\ref{prop:TL}, we have
that if $\gamma \|g_n\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$. Then,
    \begin{equation*}
        \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \gamma\ps{h_{\mu_n},g_n}_{\cH} +  \frac{\gamma^2 K}{2}\|g_n\|_{\cH}^2,
    \end{equation*}
    where $K = (\alpha^2 + M)B$. Since $\E \|g_n\|_{\cH}^2 = \E \|g_n - h_{\mu_n}\|_{\cH}^2 + \E \|h_{\mu_n}\|_{\cH}^2 = \E \|P_{\mu_n} v_n - P_{\mu_n} \nabla F\|_{\cH}^2 + \E \|h_{\mu_n}\|_{\cH}^2,$
    \begin{equation*}
        \E\cF(\mu_{n+1}) \leq \E \cF(\mu_{n}) - \gamma\E\|h_{\mu_n}\|_{\cH}^2 +  \frac{\gamma^2 K}{2}\left(\E \|h_{\mu_n}\|_{\cH}^2+\sigma^2\right).
    \end{equation*}
    Finally, if $\gamma \|g_n\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$ \textbf{almost surely},
    \begin{equation*}
        \E\cF(\mu_{n+1}) \leq \E \cF(\mu_{n}) - \gamma\left(1-\frac{\gamma K}{2}\right)I_{\st}(\mu_n|\pi) +  \frac{\gamma^2 K}{2}\sigma^2.
    \end{equation*}
    {\color{red} The condition on the step size depends on $n$. We could get rid of $n$ as in the proof of SVGD (see the induction argument in the previous section) by proving that $\|g_n\|_{\cH} \leq \phi(\E\cF(\mu_n))$, where $\phi$ is a non decreasing function. But this is not true in general, we can only have $\E \|g_n\|_{\cH} \leq \phi(\E \cF(\mu_n))$ using Talagrand's inequality, as in the previous section. \textbf{So, what bothers us is the fact that the condition on the step size must hold almost surely instead of in expectation}. We can always assume a uniform bound $\|g_n\|_{\cH} < C$ a.s. for every $n$, but we don't want to do that since our final the goal is to study a Variance Reduced version of SVSGD.}
\section{Proofs related to VR-SVSGD}
\subsection{Proof of Theorem~\ref{th:vrsvsgd}}
We now study VR-SVSGD, \textit{i.e.}, the update 
\begin{equation*}
    \mu_{n+1} = (I - \gamma g_n) \# \mu_n.
\end{equation*}

\begin{lemma}
  Let Assumptions \ref{ass:k_bounded}, \ref{ass:V_Lipschitz} and \ref{ass:T1} hold true. Let $\alpha > 1$ and choose $\gamma > 0$ such that 
  \begin{equation}
    \gamma \leq \min\left(\frac{1}{(\alpha^2 + M)B + B^2 L_n \sqrt{\frac{1}{p}-1}},\frac{\alpha-1}{\alpha B \|g_n\|_{\cH}}\right),
\end{equation}
where $L_n^2 \coloneqq \frac{1}{m}\sum_{i=1}^m \left(M+\int \|\nabla f_i(x)\|d\mu_n(x)\right)^2$.
  Then,
  \begin{equation}
    \E \cL_{n+1} \leq
     \E \cL_n - \frac{\gamma}{2}\E I_{\st}(\mu_n|\pi),
\end{equation}
where $\cL_n \coloneqq \cF(\mu_n) + \frac{\gamma}{2p}\|g_{n} - h_{\mu_{n}}\|_{\cH}^2$.
\end{lemma}
\begin{proof}
We start by applying Proposition~\ref{prop:TL}: if $\gamma \|g_n\|_{\cH} \leq \frac{\alpha-1}{\alpha B}$. Then,
    \begin{equation}
    \label{eq:TL-VR}
        \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \gamma\ps{h_{\mu_n},g_n}_{\cH} +  \frac{\gamma^2 K}{2}\|g_n\|_{\cH}^2,
    \end{equation}
    where $K = (\alpha^2 + M)B$.
    Using the polarization identity $-\ps{a,b} = \frac{1}{2}\|a-b\|^2 - \frac{1}{2}\|a\|^2 - \frac{1}{2}\|b\|^2$, the last inequality can be rewritten as
    \begin{equation}
    \label{eq:rec1}
        \cF(\mu_{n+1}) \leq \cF(\mu_{n}) - \frac{\gamma}{2}\|h_{\mu_n}\|_{\cH}^2 - \frac{\gamma}{2}(1-\gamma K)\|g_n\|_{\cH}^2 + \frac{\gamma}{2}\|g_n - h_{\mu_n}\|_{\cH}^2.
    \end{equation}
One can check by induction that for every $n$, $\E g_n = h_{\mu_n}$. In other words, the stochastic gradient $g_n$ is unbiased. Therefore, the quantity $\E \|g_n - h_{\mu_n}\|_{\cH}^2$ can be seen as a variance. Moreover, denoting $\mcG_n = \sigma(\mu_0,g_0,\ldots,\mu_n,g_n,\mu_{n+1})$,
\begin{align*}
    &\E \left(\|g_{n+1} - h_{\mu_{n+1}}\|_{\cH}^2|\mcG_{n},I_{n+1} = i\right) \\
    =& p \E \left(\|P_{\mu_{n+1}} \nabla F - \int \nabla \Phi(x)d\mu_{n+1}(x) - h_{\mu_{n+1}}\|_{\cH}^2|\mcG_{n},I_{n+1}=i\right) \\
    &+ (1-p)\E \left(\|v_n + P_{\mu_{n+1}} \nabla f_i - P_{\mu_{n}} \nabla f_i - \int \nabla \Phi(x)d\mu_{n+1}(x) - h_{\mu_{n+1}}\|_{\cH}^2|\mcG_{n},I_{n+1}=i\right).
\end{align*}
Using 
\begin{equation}
\label{eq:defh}
    P_{\mu_{n}} \nabla F - \int \nabla \Phi(x)d\mu_{n}(x) = h_{\mu_{n}},
\end{equation}
we have
\begin{align*}
    &\E \left(\|g_{n+1} - h_{\mu_{n+1}}\|_{\cH}^2|\mcG_{n},I_{n+1}=i\right) \\ 
    =& (1-p)\E \left(\|v_n + P_{\mu_{n+1}} \nabla f_i - P_{\mu_{n}} \nabla f_i - P_{\mu_{n+1}} \nabla F\|_{\cH}^2|\mcG_{n},I_{n+1}=i\right)\\
    =& (1-p)\E \left(\left\|\underbrace{\left(v_n - P_{\mu_{n}} \nabla F\right)}_{= X} + \underbrace{\left(P_{\mu_{n+1}} \nabla f_i - P_{\mu_{n}} \nabla f_i\right)}_{=a_i} - \underbrace{\left(P_{\mu_{n+1}} \nabla F - P_{\mu_{n}} \nabla F\right)}_{=\bar{a}}\right\|_{\cH}^2\Bigg|\mcG_{n},I_{n+1}=i\right)\\
    \leq& (1-p)\E (\|X\|^2_{\cH} + \ps{X,a_i-\bar{a}}_{\cH}+\|a_i-\bar{a}\|^2_{\cH} |\mcG_{n},I_{n+1}=i)\\
\end{align*}
Note that $\bar{a} = \frac{1}{m} \sum_{i=1}^m a_i$, therefore $\frac{1}{m}\sum_{i=1}^m \|a_i - \bar{a}\|_{\cH}^2 \leq \frac{1}{m}\sum_{i=1}^m \|a_i\|_{\cH}^2$. By taking the expectation w.r.t. $I_{n+1}$,
\begin{align*}
    &\E \left(\|g_{n+1} - h_{\mu_{n+1}}\|_{\cH}^2|\mcG_{n}\right) \\ 
    =&(1-p)\E\left[\|X\|_{\cH}^2 + \frac{1}{m}\sum_{i=1}^m \|a_i - \bar{a}\|_{\cH}^2\Bigg|\mcG_{n}\right]\\
    \leq&(1-p)\left[\|X\|_{\cH}^2 + \frac{1}{m}\sum_{i=1}^m  \|a_i\|_{\cH}^2\Bigg|\mcG_{n}\right]\\
    =& (1-p)\E\left[\|v_n - P_{\mu_{n}} \nabla F\|_{\cH}^2 + \frac{1}{m}\sum_{i=1}^m \|P_{\mu_{n+1}} \nabla f_i - P_{\mu_{n}} \nabla f_i\|_{\cH}^2\Bigg|\mcG_{n}\right]\\
    =& (1-p) \|g_n - h_{\mu_{n}}\|_{\cH}^2 + (1-p) \frac{1}{m}\sum_{i=1}^m \|P_{\mu_{n+1}} \nabla f_i - P_{\mu_{n}} \nabla f_i\|_{\cH}^2,\\
\end{align*}
using~\eqref{eq:defh}. Let $i \in \{1,\ldots,m\}$, we now estimate $\|P_{\mu_{n+1}} \nabla f_i - P_{\mu_{n}} \nabla f_i\|_{\cH}^2$.
\begin{align*}
    \|P_{\mu_{n+1}} \nabla f_i - P_{\mu_{n}} \nabla f_i\|_{\cH} =& \left\|\int \nabla f_i(x - \gamma g_n(x))\Phi(x - \gamma g_n(x)) - \nabla f_i(x)\Phi(x) d\mu_n(x)\right\|_{\cH}\\
    \leq& \int\left\|\nabla f_i(x - \gamma g_n(x))\Phi(x - \gamma g_n(x)) - \nabla f_i(x)\Phi(x)\right\|_{\cH}d\mu_n(x)\\
    \leq& \int\left\|\left(\nabla f_i(x - \gamma g_n(x)) - \nabla f_i(x)\right)\Phi(x - \gamma g_n(x))\right\|_{\cH}d\mu_n(x)\\
    &+ \int\left\|\left(\Phi(x - \gamma g_n(x)) - \Phi(x)\right)\nabla f_i(x)\right\|_{\cH}d\mu_n(x)\\
    \leq& \int\|\nabla f_i(x - \gamma g_n(x)) - \nabla f_i(x)\|\|\Phi(x - \gamma g_n(x))\|_{\cH_0}d\mu_n(x)\\
    &+ \int \|\Phi(x - \gamma g_n(x)) - \Phi(x)\|_{\cH_0}\|\nabla f_i(x)\|d\mu_n(x).
\end{align*}
Using \Cref{ass:k_bounded}, $\|\Phi(x)\|_{\cH_0} \leq B$ and $\Phi : \X \to \cH_0$ is $B$-Lipschitz, \textit{i.e.}, $\|\Phi(x) - \Phi(y)\|_{\cH_0} \leq B \|x-y\|$. \asnote{Check this}
Therefore, using \Cref{ass:V_Lipschitz},
\begin{align*}
    \|P_{\mu_{n+1}} \nabla f_i - P_{\mu_{n}} \nabla f_i\|_{\cH} \leq& \int MB\gamma \|g_n(x)\|d\mu_n(x)\\
    &+ \int B\gamma \|g_n(x)\|\|\nabla f_i(x)\|d\mu_n(x).
\end{align*}
Using~\eqref{eq:gbound} \asnote{Clean this}, $\|g_n(x)\| \leq B \|g_n\|_{\cH}$, therefore,
\begin{equation}
    \|P_{\mu_{n+1}} \nabla f_i - P_{\mu_{n}} \nabla f_i\|_{\cH} \leq \gamma B^2 \|g_n\|_{\cH} \left(M+\int \|\nabla f_i(x)\|d\mu_n(x)\right).
\end{equation}
We finally have
\begin{equation}
\label{eq:rec2}
    \E \left(\|g_{n+1} - h_{\mu_{n+1}}\|_{\cH}^2|\mcG_{n}\right)
    \leq (1-p) \|g_n - h_{\mu_{n}}\|_{\cH}^2 + (1-p) \gamma^2 B^4 L_n^2 \|g_n\|_{\cH}^2.
\end{equation}
Using~\eqref{eq:rec1} and~\eqref{eq:rec2},
\begin{align*}
    \E\left(\cL_{n+1}|\mcG_n\right) \leq& \cF(\mu_n) - \frac{\gamma}{2}\|h_{\mu_n}\|_{\cH}^2 - \frac{\gamma}{2}(1-\gamma K)\|g_n\|_{\cH}^2 + \frac{\gamma}{2}\|g_n - h_{\mu_n}\|_{\cH}^2\\
    &+ \frac{\gamma}{2p}\left((1-p) \|g_n - h_{\mu_{n}}\|_{\cH}^2 + (1-p) \gamma^2 B^4 L_n^2 \|g_n\|_{\cH}^2\right)\\
    =& \cL_n - \frac{\gamma}{2}\|h_{\mu_n}\|_{\cH}^2 - \frac{\gamma}{2}\|g_n\|_{\cH}^2\left(1-\gamma K-\frac{1-p}{p}\gamma^2 B^4 L_n^2\right).
\end{align*}
Since
\begin{equation}
    \gamma \leq \frac{1}{K + B^2 L_n \sqrt{\frac{1}{p}-1}},
\end{equation}
we have
\begin{equation}
    K\gamma + \frac{1-p}{p} B^4 L_n^2 \gamma^2 \leq 1.
\end{equation}
Therefore,
\begin{equation}
    \E\left(\cL_{n+1}|\mcG_n\right) \leq
     \cL_n - \frac{\gamma}{2}\|h_{\mu_n}\|_{\cH}^2,
\end{equation}
and we conclude by taking the full expectation.
\end{proof}











\section{Appendix: Lukang's Draft}

\subsection{Setup}

\begin{equation}
F=\frac{1}{n} \sum_{i=1}^{n} f_{i}, \qquad \pi \propto e^{-F }
\end{equation}
\begin{equation}
		F_{n}=\frac{1}{|I_{n}|} \sum_{i \in I_n} f_{i},  \qquad \sigma_{n} \propto e^{-F_{n}}
  \end{equation}

	\begin{equation}
		g_n^{\prime}\in \cH, \qquad g_n = \iota g \in L^2
	\end{equation}
    \begin{equation}
		\phi_{t}(x)=\left(I-t g_n\right) (x), \qquad \mu_{t}=\left(\phi_{t}\right)_{\#} \mu_{n}
  \end{equation}
    
    \begin{equation}   
	\varphi(t)=\int \log \left(\frac{\mu_{t}}{\pi}\right) d \mu_{t}
\end{equation}
  From Lemma 4.  when $t \in [0,\gamma], \gamma\leq \frac{\alpha-1}{\alpha B \left\|g_n^{\prime}\right\|_{\cH}}$, there is
  {
  	\begin{equation}
  		\varphi(\gamma)\leq \varphi(0)-\gamma \left\langle S_{\mu_n}\nabla\log \frac{\mu_n}{\pi}, g_n^{\prime} \right\rangle_{\cH} +
  		\frac{(\alpha^2+M) \gamma^2}{2} \left\langle g_n^{\prime},g_n^{\prime} \right\rangle_{\cH}
  \end{equation}}
	
  \section{PAGE}

  We define the update rule as:
    
  \begin{equation}
  g_{n+1}=\begin{cases}
        P_{\mu_{n+1}}\nabla \log \left(\frac{\mu_{n+1}}{\pi}\right) & \qquad {p},\\
        g_n+P_{\mu_{n+1}} \nabla \log \left(\frac{\mu_{n+1}}{\sigma_{I^{\prime}}}\right)-P_{\mu_{n}} \nabla \log \left(\frac{\mu_{n}}{\sigma_{I^{\prime}}}\right) & \qquad {1-p}, 
        \end{cases}   
      \end{equation}
{where  $\sigma_{I^{\prime}} \sim \exp^{-f_{I^{\prime}}}$, $f_{I^{\prime}} = \frac{1}{b^{\prime}}\sum_{i\in I^{\prime}}f_i$.}
     

     This is the mean field limit of the following algorithm SVSGD with variance reduction. Indeed,
     the update rule in $n+1$ step SVSGD is 
     \begin{equation}
     	\begin{split}
     		&\quad \frac{1}{l} \sum_{j=1}^{l}\left[k\left(x_{j}^{n+1},\cdot\right)\nabla \log\left(\sigma_{n+1}\right)\left(x_{j}^{n+1}\right)+\nabla k\left(x_j^{n+1},\cdot\right)\right] \\
     		&= \frac{1}{l} \sum_{j=1}^{l}\left[k\left(x_{j}^{n+1},\cdot\right)\left(\nabla\log\left(\sigma_n\right)\left(x_j^n\right)+\nabla \log\left(\sigma_{I^{\prime}}\right)\left(x_j^{n+1}\right)-\nabla \log\left(\sigma_{I^{\prime}}\right)\left(x_j^n\right)\right)+\nabla k\left(x_j^{n+1},\cdot\right)\right] \\
     		&= \frac{1}{l}\sum_{j=1}^{l}\left[k\left(x_j^n,\cdot\right) \nabla \log\left(\sigma_n\right)\left(x_j^n\right)+\nabla k\left(x_j^n,\cdot\right)\right] +\frac{1}{l} \sum_{j=1}^{l}\left[ k\left(x_j^{n+1},\cdot\right) \nabla \log\left(\sigma_{I^{\prime}}\right)\left(x_j^{n+1}\right)+\nabla k\left(x_j^{n+1},\cdot\right)\right]\\
     		&-\frac{1}{l} \sum_{j=1}^{l}\left[ k\left(x_j^{n},\cdot\right) \nabla \log\left(\sigma_{I^{\prime}}\right)\left(x_j^{n}\right)+\nabla k\left(x_j^{n},\cdot\right)\right]+\frac{1}{l} \sum_{j=1}^{l}\left[\left (k\left(x_j^{n+1},\cdot\right)-k\left(x_j^n,\cdot\right)\right)\left(\nabla \log\left(\sigma_n\right)\left(x_j^n\right)-\nabla \log\left(\sigma_{I^{\prime}}\right)\left(x_j^n\right)\right)\right]\\
     		&\rightarrow P_{\mu_{n}} \nabla \log \left(\frac{\mu_{n}}{\sigma_{n}}\right)+P_{\mu_{n+1}} \nabla \log \left(\frac{\mu_{n+1}}{\sigma_{I^{\prime}}}\right)-P_{\mu_{n}} \nabla \log \left(\frac{\mu_{n}}{\sigma_{I^{\prime}}}\right)+\int\left(k(\phi_{\gamma}(x),\cdot)-k(x,\cdot)\right)\nabla \log \left(\frac{\sigma_n}{\sigma_{I^{\prime}}}\right)\left(x\right)d\mu_n\left(x\right)
     	\end{split}
     \end{equation}
     So by induction,
     \begin{equation}
     	g_n(x)=P_{\mu_n}\nabla\log(\frac{\mu_n}{\sigma_n})(x)-\sum_{i=1}^{n-1}\int\left(
	   	k(\phi_{\gamma}^{(i)}(x^{\prime}),x)-k(x^{\prime},x)\right)\nabla\log(\frac{\sigma_i}{\sigma_{I^{\prime}}^{i}})(x^{\prime})d\mu_i(x^{\prime}),
     \end{equation}
 where $\nabla\log(\sigma_n)$ follows the same update rule as in PAGE, $\phi_{t}^{(i)}(x)=\left(I-t g_i\right) (x)$.\adil{(so you see (16) doesn't increase the calculation of gradient, but comparing with mean field limit of traditional PAGE, it increases the calculation of sum, I think this is as fast as the mean field limit of traditional PAGE. )}


        	
\section{Main Proof}

	By similar calculation as in PAGE, we have,
	
	
\begin{equation}
	\begin{split} \varphi(\gamma) & \leqslant \varphi(0)-\frac{\gamma}{2}\left\|S_{\mu_{n}} \nabla \log \left(\frac{\mu_{n}}{\pi}\right)\right\|_{\cH}^{2}-\left(\frac{1}{2 \gamma}-\frac{\alpha^2+M}{2}\right)  \gamma^{2}\left\|g^{\prime}_n\right\|_{\cH}^{2} \\ &+\frac{\gamma}{2}\left\|S_{\mu_{n}} \nabla \log \left(\frac{\mu_{n}}{\pi}\right)-g^{\prime}_n\right\|_{\cH}^{2}
	\end{split}
\end{equation}

here 
\begin{equation}
	\gamma\leq \frac{\alpha-1}{\alpha B \left\|g^{\prime}_n\right\|_{\cH}}.
\end{equation}
      
      Then a direct calculation now reveals that
      
      \begin{equation}
      	\begin{split}
      		 &\quad \text{E}\left[\left\|g_{n+1}^{\prime}-S_{\mu_{n+1}}\nabla \log\left(\frac{\mu_{n+1}}{\pi}\right)\right\|_{\cH}^2\right] \\
      		 &= (1-p) \text{E}\left[\left\|\right.\right. g_{n+1}^{\prime} + S_{\mu_{n+1}}\nabla \log \left(\frac{\mu_{n+1}}{\sigma_{I^{\prime}}}\right)-S_{\mu_{n}}\nabla \log \left(\frac{\mu_{n}}{\sigma_{I^{\prime}}}\right)\left.\left.\right\|_{\cH}^2\right] \\ 
      		& \leq (1-p)\text{E}\left[ \left\|\right.\right. S_{\mu_{n}}\nabla \log \left(\frac{\mu_{n}}{\pi}\right)-g_n^{\prime}\left.\left.\right\|_{\cH}^{2}\right] \\
      		& + (1-p) \text{E}\left[\left\|\right.\right. S_{\mu_{n}}\nabla \log \left(\frac{1}{\pi}\right) + S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)-S_{\mu_{n}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)+ \\ & 
      		-S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\pi}\right) \left.\left.\right\|_{\cH}^{2}\right]\\
      		& \leq (1-p)\text{E}\left[ \left\|\right.\right. S_{\mu_{n}}\nabla \log \left(\frac{\mu_{n}}{\pi}\right)-g_n^{\prime}\left.\left.\right\|_{\cH}^{2}\right] \\
      		& + (1-p) \text{E}\left[\left\|\right.\right. S_{\mu_{n}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)
      		-S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right) \left.\left.\right\|_{\cH}^{2}\right],
      	\end{split}
      \end{equation}
using $\frac{1}{n}\sum_{i = 1}^n \|a_i - \bar{a}\|^2 \leq \frac{1}{n}\sum_{i = 1}^n \|a_i\|^2$, where $\bar{a} = \frac{1}{n}\sum_{i=1}^n a_i$. 
  \section{Key Inequality}
	
		

  \begin{theorem}
    
    If $\gamma_n \cO(\bE \Phi_n) \leq C$, then
    \begin{equation}
      \bE(\Phi_{n+1})\leq \bE(\Phi_n)-\frac{1}{2}\bE\gamma_n\left\|g_n^{\prime}\right\|_{\cH}^2,
    \end{equation}
  
  where $\Phi_n:=\KL(\mu_n|\pi)+\frac{\gamma_n}{2p}\left\|g_n^{\prime}-S_{\mu_n}\nabla\log\left(\frac{\mu_n}{\sigma_n}\right)\right\|_{\cH}^2$. (In the proof,we use $\gamma$ instead of $\gamma_n$ for simplicity.)
    
    
  \end{theorem}
  
  \begin{proof}
    We need to estimate
    \begin{equation}
      \left\|S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\pi}\right)-S_{\mu_{n}}\nabla \log \left(\frac{1}{\pi}\right)\right\|_{H}^{2},
    \end{equation}
    and
      \begin{equation}
    \left\|S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)-S_{\mu_{n}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)\right\|_{H}^{2}
      \end{equation}
  
  
    First,
    
 \begin{equation}
  \begin{split}
    &S_{\mu_{n+1}} \nabla \log \left(\frac{1}{\pi}\right)-S_{\mu_{n}}\nabla \log \left(\frac{1}{\pi}\right)\\
  &=-\int k(y,\cdot)\nabla\log(\pi(y))d\mu_{n+1}(y)+\int k(\phi_{\gamma}(x),\cdot)\nabla\log(\pi(x))d\mu_n(x)\\
  &-\int k(\phi_{\gamma}(x),\cdot)\nabla\log({\pi(x)})d\mu_n(x)+\int k(x,\cdot)\nabla\log({\pi(x)})d\mu_n(x)\\
  &=I + II .
  \end{split}
\end{equation}
  
We bound each term separately.
  
\begin{equation}
  \sqN{I}=\int\int k(\phi_{\gamma}(x),\phi_{\gamma}(x^{\prime}))(\nabla\log(x)-\nabla\log(\pi(\phi_{\gamma}(x))))(\nabla\log(x^{\prime})-\nabla\log(\pi(\phi_{\gamma}(x^{\prime}))))d\mu(x)d\mu(x^{\prime})
\end{equation}
  since F is $L$-smooth, so
  
\begin{equation}
  |\nabla\log(x^{\prime})-\nabla\log(\pi(\phi_{\gamma}(x^{\prime})))|
\leq L |x-\phi_{\gamma}(x)|
	= L\gamma|g_n\left(x\right)|
\leq L B \gamma\left\|g^{\prime}_n\right\|_{\cH},
\end{equation}
  then by Cauchy-Schwartz inequality and $k(x,x)$ is bounded by C, we derive
  
  \begin{equation}
  \sqN{I}\leq C L^2 B^2\gamma^2 \left\|g^{\prime}_n\right\|_{\cH}^{2}.
  \end{equation}
  Now,
\begin{equation}
	\begin{split}
		\sqN{II}=\int\int \left(k(\phi_{\gamma}\left(x\right),\phi_{\gamma}\left(x^{\prime}\right))-k(\phi_{\gamma}\left(x\right),x^{\prime})-k(x,\phi_{\gamma}\left(x^{\prime}\right))+k(x,x^{\prime})
		\right)\nabla\log \left(\pi\left(x\right)\right)\nabla\log \left(\pi\left(x^{\prime}\right)\right) d\mu_n(x)d\mu_n(x^{\prime})
	\end{split}
\end{equation}
 

  
  We need to know the order of $|k(\phi_{\gamma}(x),\phi_{\gamma}(x^{\prime}))-k(\phi_{\gamma}(x),x^{\prime})-k(x,\phi_{\gamma}(x^{\prime}))+k(x,x^{\prime})|$, in the following we assume supremum norm of the second order derivatives of $k$ are bounded by C, then by mean value theorem, we have
  
  \begin{equation}
  	\begin{split}
  		&\quad |k(\phi_{\gamma}(x),\phi_{\gamma}(x^{\prime}))-k(\phi_{\gamma}(x),x^{\prime})-k(x,\phi_{\gamma}(x^{\prime}))+k(x,x^{\prime})|\\
  		& \leq |\langle \phi_{\gamma}\left(x^{\prime}\right)-x^{\prime},\nabla_2 k\left(\phi_{\gamma}\left(x\right),\phi_{\theta_1}(x^{\prime})\right)\rangle-\langle \phi_{\gamma}\left(x^{\prime}\right)-x^{\prime},\nabla_2 k\left(x,\phi_{\theta_2}(x^{\prime})\right)\rangle|\\
  		&\leq
		C|\phi_{\gamma}(x)-x|(|\phi_{\gamma}(x)-x|+|\phi_{\theta_1}(x^{\prime})-\phi_{\theta_2}(x^{\prime})|) \\
		& \leq 2C B^2 \gamma^2 \left\|g_n^{\prime}\right\|_{\cH}^2
  	\end{split}
  \end{equation}


  Then, we know,
  \begin{equation}
  \sqN{II} \leq 2CB^2\left(\int|\nabla\log(\pi(x))|d\mu_n(x)\right)^2\gamma^2\left\|g_n^{\prime}\right\|_{\cH}^2
  \end{equation}

So
\begin{equation}
	\left\|S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\pi}\right)-S_{\mu_{n}}\nabla \log \left(\frac{1}{\pi}\right)\right\|_{H}^{2}\leq CB^2\left(L^2+\left(\int|\nabla\log(\pi(x))|d\mu_n(x)\right)^2\right)\gamma^2\left\|g_n^{\prime}\right\|_{\cH}^2 ,
\end{equation}
  
  similarly, we have 
  
  \begin{equation}
  	\left\|S_{\mu_{n+1}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)-S_{\mu_{n}}\nabla \log \left(\frac{1}{\sigma_{I^{\prime}}}\right)\right\|_{H}^{2}\leq CB^2\left(L^2+\left(\int|\nabla\log(\sigma_{I^{\prime}}(x))|d\mu_n(x)\right)^2\right)\gamma^2\left\|g_n^{\prime}\right\|_{\cH}^2 ,
  \end{equation}
  
  %\begin{equation}
  %	\begin{split}
  	%	& \quad \left\|\int\left(k(\phi_{\gamma}(x),\cdot)-k(x,\cdot)\right)\nabla \log \left(\frac{\sigma_n}{\sigma_{I^{\prime}}}\right)\left(x\right)d\mu_n\left(x\right)\right\|_H^2\\
  	%	&\!\!\!\!\!=\int\int\left(k\left(\phi_{\gamma}\left(x\right),\phi_{\gamma}\left(x^{\prime}\right)\right)-k\left(\phi_{\gamma}\left(x\right),x^{\prime}\right)-k\left(x,\phi_{\gamma}\left(x^{\prime}\right)\right)+k\left(x,x^{\prime}\right)
  	%	\right)\nabla\log \left(\frac{\sigma_{n}}{\sigma_{I^{\prime}}}\right)\left(x\right)\nabla\log \left(\frac{\sigma_n}{\sigma_{I^{\prime}}}\right)\left(x^{\prime}\right) d\mu_n(x)d\mu_n(x^{\prime})\\
  	%	&\!\!\!\!\!\leq C_3\left(\int |\nabla \log\left(\frac{\sigma_n}{\sigma_{I^{\prime}}}\right)|d\mu_n\right)^2\gamma^2\left\|S_{\mu_n}\nabla\log(\frac{\mu_n}{\sigma_n})\right\|_H^2
  	%\end{split}
  	%\end{equation}
  
  
  So if $\gamma$ is small enough, s.t 
  
  \begin{equation}
  	\begin{split}
  		&\quad\frac{\gamma}{2p}\left(CB^2\left(L^2+\left(\int|\nabla\log(\sigma_{I^{\prime}}(x))|d\mu_n(x)\right)^2\right)+CB^2\left(L^2+\left(\int|\nabla\log(\pi(x))|d\mu_n(x)\right)^2\right)\right)\\
  		& -\left(\frac{1}{2\gamma}-\frac{\alpha^2+M}{2}\right)\leq 0,
  	\end{split}
    \end{equation}
 by choosing \begin{equation}
 	\gamma^2\leq \frac{\frac{\alpha^2+M}{2}}{1+\frac{2CB^2L^2+CB^2\left(\left(\int|\nabla\log(\pi)(x)|d\mu_n(x)\right)^2+\left(\int|\nabla\log(\sigma_{I^{\prime}})(x)|d\mu_n(x)\right)^2\right)}{2p}},
 \end{equation} then last condition holds. 
 
  Then
  \begin{equation}
    \bE(\Phi_{n+1}) \leq \bE(\Phi_n)-\frac{1}{2}\bE (\gamma \left\|g_n^{\prime}\right\|_{\cH}^2)
  \end{equation}
  \end{proof}
If we let
\begin{equation}
	\bar{\gamma_n}^2=\frac{(\alpha-1)^2}{\alpha^2B^2\left\|g_n^{\prime}\right\|_{\cH}^2},
\end{equation}  
\begin{equation}
	\tilde{\gamma_n}^2= \frac{\frac{\alpha^2+M}{2}}{1+\frac{2CB^2L^2+CB^2\left(\left(\int|\nabla\log(\pi)(x)|d\mu_n(x)\right)^2+\left(\int|\nabla\log(\sigma_{I^{\prime}})(x)|d\mu_n(x)\right)^2\right)}{2p}},
\end{equation}
then $\gamma_n=\min\left(\bar{\gamma}_n,\tilde{\gamma_n}\right)$ satisfies (19) and (34). Now we need to bound the denominators in (36) and (37).

\begin{lemma}
	If there existing a constant $a>0$ and a point $x_0$ such that $\int e^{ad(x_0,x)^2}d\pi(x)<+\infty$, then we have the following Talagrand inequality
	\begin{equation}
		W_1\left(\mu,\pi\right)\leq \lambda\sqrt{KL\left(\mu|\pi\right)}
	\end{equation}
\end{lemma}
  
\begin{proof}
	see Villani...
\end{proof}

\begin{lemma}
	\text{E}$\left(\left\|g_n^{\prime}\right\|_{\cH}^2\right)$ and \text{E}$\left(\left(\int|\nabla\log(\pi)(x)|d\mu_n(x)\right)^2+\left(\int|\nabla\log(\sigma_{I^{\prime}})(x)|d\mu_n(x)\right)^2\right)$ are finite.
\end{lemma}

\begin{proof}
	\begin{equation}
		\begin{split}
			\text{E}\left(\left\|g_n^{\prime}\right\|_{\cH}^2\right)& \leq \text{E}\left(\left\|g_{n}^{\prime}-S_{\mu_n}\nabla\log(\frac{\mu_n}{\pi})\right\|_{\cH}+\left\|S_{\mu_n}\nabla\log(\frac{\mu_n}{\pi})\right\|_{\cH}\right)^2\\
			&\leq \text{E}\left(\left\|g_{n}^{\prime}-S_{\mu_n}\nabla\log(\frac{\mu_n}{\pi})\right\|_{\cH}+\left\|S_{\mu_n}\nabla\log(\mu_n)\right\|_{\cH}+\left\|S_{\mu_n}\nabla\log(\pi)\right\|_{\cH}\right)^2\\
			&\leq 3 \text{E}\left(\left\|g_{n}^{\prime}-S_{\mu_n}\nabla\log(\frac{\mu_n}{\pi})\right\|_{\cH}^2\right)+3 \text{E}\left(\left\|S_{\mu_n}\nabla\log(\mu_n)\right\|_{\cH}^2\right)+3 \text{E}\left(\left\|S_{\mu_n}\nabla\log(\pi)\right\|_{\cH}^2\right).
		\end{split}
	\end{equation}

We know the first term on the right hand side is bounded by $3 \Phi_0$, the second term is bounded
 by $3C$ since the derivative of $k$ is bounded by $C$, the third term is bounded by
  $6C\left(|\nabla\log(\pi(0))|\right)^2+6CM\text{E}\left(\int|x|d\mu_n(x)\right)^2$ since the $M$-smoothness of $\log(\pi(x))$.
  
  Similarly, we have 
  \begin{equation}
  	\begin{split}
  		&\quad\text{E}\left(\left(\int|\nabla\log(\pi)(x)|d\mu_n(x)\right)^2+\left(\int|\nabla\log(\sigma_{I^{\prime}})(x)|d\mu_n(x)\right)^2\right) \\
  		&\leq 2\left(|\nabla\log(\pi(0))|\right)^2+2\max_{I^{\prime}}\left(|\nabla\log(\sigma_{I^{\prime}}(0))|\right)^2+4M\text{E}\left(\int|x|d\mu_n(x)\right)^2
  	\end{split}
  \end{equation}
  From Talagrand inequality, we have 
  \begin{equation}
  	\text{E}\left(W_1(\mu_n,\delta_0)-W_1(\delta_0,\pi)\right)^2\leq \text{E}\left(W_1^2(\mu_n,\pi)\right)\leq \lambda\text{E}\left(KL(\mu_n|\pi)\right)\leq \lambda \Phi_0
  \end{equation}
so by Young inequality,
\begin{equation}
	\begin{split}
	\text{E}\left(W_1^2(\mu_n,\delta_0)\right)&\leq \lambda \Phi_0-W_1^2(\delta_0,\pi)+2W_1(\delta_0,\pi)\text{E}\left(W_1(\mu_n,\delta_0)\right)\\
	& \leq \lambda \Phi_0-W_1^2(\delta_0,\pi)+8W_1^2(\delta_0,\pi)+\frac{(\text{E}(W_1(\mu_n,\delta_0)))^2}{4}\\
	& \leq \lambda \Phi_0+7W_1^2(\delta_0,\pi)+\frac{\text{E}(W_1^2(\mu_n,\delta_0))}{4}
	\end{split}
\end{equation}
Finally, we have

\begin{equation}
	\text{E}\left(\int|x|d\mu_n(x)\right)^2=\text{E}(W_1^2(\mu_n,\delta_0))\leq \frac{4}{3}\left(\lambda \Phi_0+7W_1^2(\delta_0,\pi)\right).
\end{equation}
In the end,
\begin{equation}
	\text{E}\left(\left\|g_n^{\prime}\right\|_{\cH}^2\right)\leq 3\Phi_0+3C+6C(|\nabla\log(\pi(0))|)^2+8CM\left(\lambda \Phi_0+7W_1^2(\delta_0,\pi)\right),
\end{equation}
\begin{equation}
	\begin{split}
	&\quad\text{E}\left(\left(\int|\nabla\log(\pi)(x)|d\mu_n(x)\right)^2+\left(\int|\nabla\log(\sigma_{I^{\prime}})(x)|d\mu_n(x)\right)^2\right)\\
	&\leq 2\left(|\nabla\log(\pi(0))|\right)^2+2\max_{I^{\prime}}\left(|\nabla\log(\sigma_{I^{\prime}}(0))|\right)^2+\frac{16}{3}M\left(\lambda \Phi_0+7W_1^2(\delta_0,\pi)\right).
	\end{split}
\end{equation}
 
\end{proof}

From the definition of $\bar{\gamma_n}$, $\tilde{\gamma_n}$ and Lemma 8., we now have
\begin{equation}
	\text{E}\left(\frac{1}{\bar{\gamma_n^2}}\right)\leq C_1,
\end{equation}


\begin{equation}
	\text{E}\left(\frac{1}{\tilde{\gamma_n^2}}\right)\leq C_2,
\end{equation}

So 
\begin{equation}
	\text{E}\left(\frac{1}{\gamma_n^2}\right)\leq \text{E}\left(\frac{1}{\tilde{\gamma_n^2}}\right)+\text{E}\left(\frac{1}{\bar{\gamma_n^2}}\right)\leq C_1+C_2.
\end{equation}
(We can calculate the exact form of $C_1,C_2$, since they are too complicated, we ignore here.)

\begin{lemma}
		We have no less than 0.96 chance such that $\gamma_n\geq \frac{1}{5\sqrt{C_1+C_2}}$, in other words,
		$\text{E}(\gamma_n)\geq \frac{0.96}{5\sqrt{C_1+C_2}}.$
\end{lemma}

\begin{proof}
	if we have more than $\frac{1}{25}$ chance such that $\gamma_n\leq \frac{1}{5\sqrt{C_1+C_2}} $, then $\text{E}(\frac{1}{\gamma_n^2})>\frac{1}{25}25(C_1+C_2)=C_1+C_2$, which contradicts  (48).
\end{proof}


\begin{lemma}
	The expectation of the number $\#$of $=\{k|\gamma_k\geq \frac{1}{5\sqrt{C_1+C_2}},k\leq n\}$ is at least $0.96n$ and the posibility such that this number is bigger than $0.5n$ is at least 0.92.
\end{lemma}

\begin{proof}
	The first argument is easy. If the chance that this number less than $0.5n$ is bigger than 0.08,
	then $\text{E}\left(\#\right)<0.08\times 0.5n+n(1-0.08)=0.96n$ which contradicts the first argument.
\end{proof}

We define $I_{\gamma_n\geq \frac{1}{5\sqrt{C_1+C_2}}}=1$, if $\gamma_n\geq \frac{1}{5\sqrt{C_1+C_2}}$, else $I_{\gamma_n\geq \frac{1}{5\sqrt{C_1+C_2}}}=0$.
So we have 
\begin{equation}
	\begin{split}
	\Phi_0\geq\sum_{i=1}^n\text{E}\left(\frac{\gamma_i}{2}I^2_{Stein}(\mu_i|\pi)\right)&\geq \sum_{i=1}^n\text{E}\left(I_{\gamma_i\geq \frac{1}{5\sqrt{C_1+C_2}}}\frac{\gamma_i}{2}I^2_{Stein}(\mu_i|\pi)\right)\\
	& \geq\frac{1}{10\sqrt{C_1+C_2}}\sum_{i=1}^n\text{E}\left(I_{\gamma_i\geq \frac{1}{5\sqrt{C_1+C_2}}}I^2_{Stein}(\mu_i|\pi)\right)
	\end{split}
\end{equation}
We also know that 
\begin{equation}
	\min_{j\in \{k|\gamma_k\geq \frac{1}{5\sqrt{C_1+C_2}},k\leq n\}}I^2_{Stein}(\mu_j|\pi)\leq I^2_{Stein}(\mu_i|\pi),
\end{equation}
for every $i\in \{k|\gamma_k\geq \frac{1}{5\sqrt{C_1+C_2}},k\leq n\}$, then we know
\begin{equation}
	\begin{split}
		&\quad\text{E}\left(\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\right)\\
		&\leq\text{E}\left(\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\min_{j\in \{k|\gamma_k\geq \frac{1}{5\sqrt{C_1+C_2}},k\leq n\}}I^2_{Stein}(\mu_j|\pi)\right)\\
		&\leq \text{E}\left(I_{\gamma_i\geq \frac{1}{5\sqrt{C_1+C_2}}}I^2_{Stein}(\mu_i|\pi)\right)
	\end{split}
\end{equation}
combine (49) and (51), we derive that
\begin{equation}
	\text{E}\left(\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\right)\leq 10\sqrt{C_1+C_2}\Phi_0.
\end{equation}

so 
\begin{equation}
	\begin{split}
	&\quad \text{E}\left(\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\Big|\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\geq 0.5n\right)\leq \frac{10\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}
	\end{split}
\end{equation}
\begin{lemma}
	We have more than 0.9108 chance such that $\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\leq \frac{1000\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}$.
\end{lemma}
\begin{proof}
	If not , then we have more than 0.01 chance such that $\text{E}\left(\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\Big|\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\geq 0.5n\cap \mathcal{F}\right)> \frac{1000\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}$, then  
	\begin{equation}
		\begin{split}
			\text{E}\left(\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\Big|\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\geq 0.5n\right)&>0.01\times \frac{1000\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}\\
			&= \frac{10\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}
		\end{split}
	\end{equation}
which contradicts (53). So we have more than 0.99 chance such that 
$\text{E}\left(\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\Big|\#\{j|\gamma_j\geq \frac{1}{5\sqrt{C_1+C_2}},j\leq n\}\geq 0.5n\cap \mathcal{F}\right)\leq \frac{1000\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}$, so we have more than $0.99\times 0.92=0.9108$ chance such that  $\min_{j\leq n}I^2_{Stein}(\mu_j|\pi)\leq \frac{1000\sqrt{C_1+C_2}\Phi_0}{0.92\times 0.5n}$
\end{proof}
























































\end{document}
